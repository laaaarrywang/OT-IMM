{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Time-Indexed RealNVP Implementation\n",
    "\n",
    "This notebook comprehensively tests the time-indexed RealNVP flow T(t, x) for:\n",
    "1. **Invertibility**: T^{-1}(t, T(t, x)) = x\n",
    "2. **Differentiability w.r.t. time**: ∂T/∂t exists and is smooth\n",
    "3. **Differentiability w.r.t. input**: ∂T/∂x exists with correct Jacobian\n",
    "4. **Log-determinant consistency**: log|det(∂T/∂x)| matches forward and inverse\n",
    "5. **Multiple scales**: From tiny 8×8 to ImageNet 224×224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "================================================================================\n",
      "GPUs available: 1\n",
      "GPU 0: NVIDIA H100 80GB HBM3, 79.2 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from realnvp import TimeIndexedRealNVP, create_vector_flow, create_mnist_flow,create_cifar10_flow, create_imagenet_flow, create_imagenet_flow_stable\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"GPUs available:\", torch.cuda.device_count())\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    props = torch.cuda.get_device_properties(i)\n",
    "    print(f\"GPU {i}: {props.name}, {props.total_memory/1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Count Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    n = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    mb = n * 8 / 1e6  # float32\n",
    "    return n, mb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check Invertibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def check_invertibility(model, x, t, name, atol=1e-5, rtol=1e-5):\n",
    "    model.eval()\n",
    "    y, ld1 = model(x, t)\n",
    "    x_rec, ld2 = model.inverse(y, t)\n",
    "\n",
    "    diff = (x_rec - x).reshape(x.shape[0], -1)\n",
    "    l2 = diff.norm(dim=1)\n",
    "    linf = diff.abs().max(dim=1).values\n",
    "    rel = l2 / (x.reshape(x.shape[0], -1).norm(dim=1) + 1e-12)\n",
    "\n",
    "    # logdet from inverse should be the negative of forward\n",
    "    logdet_consistency = (ld1 + ld2).abs()\n",
    "\n",
    "    print(f\"\\n[{name}] Invertibility check\")\n",
    "    print(f\"  ||x_rec - x||_2    : mean {l2.mean():.3e} | max {l2.max():.3e}\")\n",
    "    print(f\"  ||x_rec - x||_inf  : mean {linf.mean():.3e} | max {linf.max():.3e}\")\n",
    "    print(f\"  rel L2 error       : mean {rel.mean():.3e} | max {rel.max():.3e}\")\n",
    "    print(f\"  |logdet + logdet^-1|: mean {logdet_consistency.mean():.3e} | max {logdet_consistency.max():.3e}\")\n",
    "\n",
    "    ok = (l2.max() < atol + rtol * x.abs().max()) and (logdet_consistency.max() < 1e-8)    \n",
    "    print(f\"  PASS: {ok}\")\n",
    "    return ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check Differentiability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_differentiability_t(model, x, name, eps_list=(1e-1, 3e-2, 1e-2, 3e-3, 1e-3)):\n",
    "    \"\"\"Check differentiability w.r.t. time using autograd vs finite differences\"\"\"\n",
    "    model.eval()\n",
    "    B = x.shape[0]\n",
    "    device = x.device\n",
    "    D = x.view(B, -1).shape[1]  # Total dimension\n",
    "    \n",
    "    # random t in [0,1]\n",
    "    t = torch.rand(B, device=device, dtype=x.dtype, requires_grad=True)\n",
    "\n",
    "    # Autograd gradient of sum(y_i) w.r.t. t_i (per-sample)\n",
    "    y, _ = model(x, t)\n",
    "    f = y.view(B, -1).sum(dim=1)          # [B]\n",
    "    g_aut, = torch.autograd.grad(f.sum(), t, create_graph=False)  # [B]\n",
    "    \n",
    "    # Per-element gradients (normalized by dimension)\n",
    "    g_aut_per_elem = g_aut / D\n",
    "\n",
    "    print(f\"\\n[{name}] Differentiability in time: autograd vs finite-difference\")\n",
    "    print(f\"  Dimension: {D:,}\")\n",
    "    print(f\"  Total grad stats: mean {g_aut.mean().item():.3e} | std {g_aut.std().item():.3e}\")\n",
    "    print(f\"  Per-elem grad:    mean {g_aut_per_elem.mean().item():.3e} | std {g_aut_per_elem.std().item():.3e}\")\n",
    "\n",
    "    # Finite differences\n",
    "    with torch.no_grad():\n",
    "        for eps in eps_list:\n",
    "            t_plus = (t.detach() + eps).clamp(0.0, 1.0)\n",
    "            y_plus, _ = model(x, t_plus)\n",
    "            fd = (y_plus - y.detach()) / eps        # same shape as y\n",
    "            fd_sum = fd.view(B, -1).sum(dim=1)      # [B]\n",
    "\n",
    "            abs_err = (fd_sum - g_aut.detach()).abs()\n",
    "            rel_err = abs_err / (g_aut.detach().abs() + 1e-12)\n",
    "            \n",
    "            # Per-element errors\n",
    "            abs_err_per_elem = abs_err / D\n",
    "            rel_err_per_elem = rel_err  # relative error doesn't change with normalization\n",
    "\n",
    "            print(f\"  eps={eps:>7.1e} | total_err {abs_err.mean():.3e} | \"\n",
    "                  f\"per_elem_err {abs_err_per_elem.mean():.3e} | rel_err {rel_err_per_elem.mean():.3e}\")\n",
    "\n",
    "def check_differentiability_x(model, x_template, name, eps_list=(1e-3, 3e-4, 1e-4, 3e-5, 1e-5)):\n",
    "    \"\"\"Check differentiability w.r.t. input x using autograd vs finite differences\"\"\"\n",
    "    model.eval()\n",
    "    B = x_template.shape[0]\n",
    "    device = x_template.device\n",
    "    D = x_template.view(B, -1).shape[1]  # Total dimension\n",
    "    \n",
    "    # Create input x that requires grad\n",
    "    x = x_template.clone().detach().requires_grad_(True)\n",
    "    t = torch.rand(B, device=device, dtype=x.dtype)  # Fixed time for this test\n",
    "    \n",
    "    # Forward pass\n",
    "    y, logdet = model(x, t)\n",
    "    \n",
    "    # Compute scalar output for differentiation (sum of all outputs)\n",
    "    f = y.view(B, -1).sum(dim=1).sum()  # Single scalar\n",
    "    \n",
    "    # Autograd gradient w.r.t. x\n",
    "    g_aut = torch.autograd.grad(f, x, create_graph=False)[0]  # Same shape as x\n",
    "    g_aut_flat = g_aut.view(B, -1)  # [B, D]\n",
    "    \n",
    "    # Per-element gradient statistics\n",
    "    g_aut_per_elem = g_aut_flat.abs().mean()\n",
    "\n",
    "    print(f\"\\n[{name}] Differentiability in input: autograd vs finite-difference\")\n",
    "    print(f\"  Input shape: {x.shape}\")\n",
    "    print(f\"  Total dimensions: {D:,}\")\n",
    "    print(f\"  Grad stats: mean {g_aut_flat.mean().item():.3e} | std {g_aut_flat.std().item():.3e}\")\n",
    "    print(f\"  Grad magnitude: mean |∇x| {g_aut_per_elem.item():.3e}\")\n",
    "\n",
    "    # Finite differences - test a few random directions\n",
    "    with torch.no_grad():\n",
    "        # Create random perturbation directions (normalized)\n",
    "        directions = torch.randn_like(x)\n",
    "        directions = directions / directions.view(B, -1).norm(dim=1, keepdim=True).view(B, *([1] * (x.dim()-1)))\n",
    "        \n",
    "        for eps in eps_list:\n",
    "            # Perturb input\n",
    "            x_plus = x.detach() + eps * directions\n",
    "            \n",
    "            # Forward pass with perturbed input\n",
    "            y_plus, logdet_plus = model(x_plus, t)\n",
    "            f_plus = y_plus.view(B, -1).sum(dim=1).sum()\n",
    "            \n",
    "            # Finite difference approximation\n",
    "            fd_scalar = (f_plus - f.detach()) / eps\n",
    "            \n",
    "            # Directional derivative from autograd: ∇f · direction\n",
    "            directional_grad = (g_aut.detach() * directions).view(B, -1).sum(dim=1).sum()\n",
    "            \n",
    "            # Compare\n",
    "            abs_err = abs(fd_scalar - directional_grad)\n",
    "            rel_err = abs_err / (abs(directional_grad) + 1e-12)\n",
    "            \n",
    "            print(f\"  eps={eps:>7.1e} | abs_err {abs_err:.3e} | rel_err {rel_err:.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset-specific runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_run_imagenet(device=\"cpu\"):\n",
    "    print(\"\\n=== IMAGENET-LIKE IMAGE FLOW ===\")\n",
    "    model = create_imagenet_flow(\n",
    "        resolution=128,  # Smaller than default 224 for memory efficiency\n",
    "        num_layers=6, time_embed_dim=512,\n",
    "        img_base_channels=256, img_blocks=3, img_groups=32,\n",
    "        img_log_scale_clamp=10.0, use_permutation=True\n",
    "    ).to(device)\n",
    "    nparams, mb = count_params(model)\n",
    "    print(f\"Params: {nparams:,} ({mb:.2f} MB)\")\n",
    "    x = torch.randn(4, 3, 128, 128, device=device, dtype=torch.float64)  # Very small batch for memory\n",
    "    t = torch.rand(4, device=device, dtype=torch.float64)\n",
    "    check_invertibility(model, x, t, \"ImageNet (3x128x128)\")\n",
    "    check_differentiability_t(model, x, \"ImageNet (3x128x128)\")\n",
    "    check_differentiability_x(model, x, \"ImageNet (3x128x128)\")\n",
    "\n",
    "def quick_run_imagenet_full(device=\"cpu\"):\n",
    "    print(\"\\n=== FULL IMAGENET FLOW ===\")\n",
    "    model = create_imagenet_flow(\n",
    "        resolution=224,  # Full resolution\n",
    "        num_layers=4, time_embed_dim=256,\n",
    "        img_base_channels=128, img_blocks=4, img_groups=32,\n",
    "        img_log_scale_clamp=10.0, use_permutation=True\n",
    "    ).to(device)\n",
    "    nparams, mb = count_params(model)\n",
    "    print(f\"Params: {nparams:,} ({mb:.2f} MB)\")\n",
    "    x = torch.randn(6, 3, 224, 224, device=device, dtype=torch.float64)\n",
    "    t = torch.rand(6, device=device, dtype=torch.float64)\n",
    "    check_invertibility(model, x, t, \"ImageNet (3x224x224)\")\n",
    "    check_differentiability_t(model, x, \"ImageNet (3x224x224)\")\n",
    "    check_differentiability_x(model, x, \"ImageNet (3x224x224)\")\n",
    "\n",
    "def quick_run_vector(device=\"cpu\"):\n",
    "    print(\"\\n=== VECTOR FLOW ===\")\n",
    "    model = create_vector_flow(\n",
    "        dim=32, num_layers=6, time_embed_dim=64,\n",
    "        hidden=512, mlp_blocks=3, activation=\"gelu\",\n",
    "        use_layernorm=False, use_permutation=True\n",
    "    ).to(device)\n",
    "    nparams, mb = count_params(model)\n",
    "    print(f\"Params: {nparams:,} ({mb:.2f} MB)\")\n",
    "    x = torch.randn(8, 32, device=device, dtype=torch.float64)\n",
    "    t = torch.rand(8, device=device, dtype=torch.float64)\n",
    "    check_invertibility(model, x, t, \"vector(32)\")\n",
    "    check_differentiability_t(model, x, \"vector(32)\")\n",
    "    check_differentiability_x(model, x, \"vector(32)\")\n",
    "\n",
    "def quick_run_mnist_like(device=\"cpu\"):\n",
    "    print(\"\\n=== MNIST-LIKE IMAGE FLOW ===\")\n",
    "    model = create_mnist_flow(\n",
    "        image_mode=True, num_layers=6, time_embed_dim=128,\n",
    "        img_base_channels=96, img_blocks=3, img_groups=32,\n",
    "        img_log_scale_clamp=10.0, use_permutation=True\n",
    "    ).to(device)\n",
    "    nparams, mb = count_params(model)\n",
    "    print(f\"Params: {nparams:,} ({mb:.2f} MB)\")\n",
    "    x = torch.randn(4, 1, 28, 28, device=device, dtype=torch.float64)\n",
    "    t = torch.rand(4, device=device, dtype=torch.float64)\n",
    "    check_invertibility(model, x, t, \"MNIST (1x28x28)\")\n",
    "    check_differentiability_t(model, x, \"MNIST (1x28x28)\")\n",
    "    check_differentiability_x(model, x, \"MNIST (1x28x28)\")\n",
    "\n",
    "def quick_run_cifar(device=\"cpu\"):\n",
    "    print(\"\\n=== CIFAR-10 IMAGE FLOW ===\")\n",
    "    model = create_cifar10_flow(\n",
    "        num_layers=8, time_embed_dim=128,\n",
    "        img_base_channels=128, img_blocks=4, img_groups=32,\n",
    "        img_log_scale_clamp=10.0, use_permutation=True\n",
    "    ).to(device)\n",
    "    nparams, mb = count_params(model)\n",
    "    print(f\"Params: {nparams:,} ({mb:.2f} MB)\")\n",
    "    x = torch.randn(4, 3, 32, 32, device=device, dtype=torch.float64)  # keep small batch for speed\n",
    "    t = torch.rand(4, device=device, dtype=torch.float64)\n",
    "    check_invertibility(model, x, t, \"CIFAR (3x32x32)\")\n",
    "    check_differentiability_t(model, x, \"CIFAR (3x32x32)\")\n",
    "    check_differentiability_x(model, x, \"CIFAR (3x32x32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "=== VECTOR FLOW ===\n",
      "Params: 3,499,200 (27.99 MB)\n",
      "\n",
      "[vector(32)] Invertibility check\n",
      "  ||x_rec - x||_2    : mean 3.780e-16 | max 6.504e-16\n",
      "  ||x_rec - x||_inf  : mean 2.637e-16 | max 4.441e-16\n",
      "  rel L2 error       : mean 7.216e-17 | max 1.144e-16\n",
      "  |logdet + logdet^-1|: mean 1.110e-15 | max 3.553e-15\n",
      "  PASS: True\n",
      "\n",
      "[vector(32)] Differentiability in time: autograd vs finite-difference\n",
      "  Dimension: 32\n",
      "  Total grad stats: mean 2.419e+00 | std 7.577e+01\n",
      "  Per-elem grad:    mean 7.561e-02 | std 2.368e+00\n",
      "  eps=1.0e-01 | total_err 5.731e+01 | per_elem_err 1.791e+00 | rel_err 9.958e-01\n",
      "  eps=3.0e-02 | total_err 5.777e+01 | per_elem_err 1.805e+00 | rel_err 9.805e-01\n",
      "  eps=1.0e-02 | total_err 5.624e+01 | per_elem_err 1.757e+00 | rel_err 9.656e-01\n",
      "  eps=3.0e-03 | total_err 6.036e+01 | per_elem_err 1.886e+00 | rel_err 1.199e+00\n",
      "  eps=1.0e-03 | total_err 6.608e+01 | per_elem_err 2.065e+00 | rel_err 1.562e+00\n",
      "\n",
      "[vector(32)] Differentiability in input: autograd vs finite-difference\n",
      "  Input shape: torch.Size([8, 32])\n",
      "  Total dimensions: 32\n",
      "  Grad stats: mean 6.950e-01 | std 1.127e-01\n",
      "  Grad magnitude: mean |∇x| 6.950e-01\n",
      "  eps=1.0e-03 | abs_err 1.527e-06 | rel_err 1.682e-06\n",
      "  eps=3.0e-04 | abs_err 4.582e-07 | rel_err 5.046e-07\n",
      "  eps=1.0e-04 | abs_err 1.527e-07 | rel_err 1.682e-07\n",
      "  eps=3.0e-05 | abs_err 4.559e-08 | rel_err 5.021e-08\n",
      "  eps=1.0e-05 | abs_err 1.480e-08 | rel_err 1.630e-08\n",
      "\n",
      "=== MNIST-LIKE IMAGE FLOW ===\n",
      "Params: 3,463,500 (27.71 MB)\n",
      "\n",
      "[MNIST (1x28x28)] Invertibility check\n",
      "  ||x_rec - x||_2    : mean 3.810e-14 | max 3.948e-14\n",
      "  ||x_rec - x||_inf  : mean 6.273e-15 | max 6.661e-15\n",
      "  rel L2 error       : mean 1.356e-15 | max 1.406e-15\n",
      "  |logdet + logdet^-1|: mean 5.684e-14 | max 5.684e-14\n",
      "  PASS: True\n",
      "\n",
      "[MNIST (1x28x28)] Differentiability in time: autograd vs finite-difference\n",
      "  Dimension: 784\n",
      "  Total grad stats: mean 1.134e+05 | std 3.800e+05\n",
      "  Per-elem grad:    mean 1.446e+02 | std 4.846e+02\n",
      "  eps=1.0e-01 | total_err 3.125e+05 | per_elem_err 3.986e+02 | rel_err 9.951e-01\n",
      "  eps=3.0e-02 | total_err 3.077e+05 | per_elem_err 3.925e+02 | rel_err 9.666e-01\n",
      "  eps=1.0e-02 | total_err 3.059e+05 | per_elem_err 3.902e+02 | rel_err 9.585e-01\n",
      "  eps=3.0e-03 | total_err 2.881e+05 | per_elem_err 3.674e+02 | rel_err 9.530e-01\n",
      "  eps=1.0e-03 | total_err 3.040e+05 | per_elem_err 3.877e+02 | rel_err 8.418e-01\n",
      "\n",
      "[MNIST (1x28x28)] Differentiability in input: autograd vs finite-difference\n",
      "  Input shape: torch.Size([4, 1, 28, 28])\n",
      "  Total dimensions: 784\n",
      "  Grad stats: mean 1.098e+00 | std 4.742e-01\n",
      "  Grad magnitude: mean |∇x| 1.099e+00\n",
      "  eps=1.0e-03 | abs_err 1.335e-04 | rel_err 4.826e-03\n",
      "  eps=3.0e-04 | abs_err 4.005e-05 | rel_err 1.448e-03\n",
      "  eps=1.0e-04 | abs_err 1.335e-05 | rel_err 4.826e-04\n",
      "  eps=3.0e-05 | abs_err 4.005e-06 | rel_err 1.447e-04\n",
      "  eps=1.0e-05 | abs_err 1.382e-06 | rel_err 4.996e-05\n",
      "\n",
      "=== CIFAR-10 IMAGE FLOW ===\n",
      "Params: 10,611,760 (84.89 MB)\n",
      "\n",
      "[CIFAR (3x32x32)] Invertibility check\n",
      "  ||x_rec - x||_2    : mean 1.498e-13 | max 1.642e-13\n",
      "  ||x_rec - x||_inf  : mean 1.227e-14 | max 1.488e-14\n",
      "  rel L2 error       : mean 2.690e-15 | max 2.956e-15\n",
      "  |logdet + logdet^-1|: mean 1.137e-13 | max 2.274e-13\n",
      "  PASS: True\n",
      "\n",
      "[CIFAR (3x32x32)] Differentiability in time: autograd vs finite-difference\n",
      "  Dimension: 3,072\n",
      "  Total grad stats: mean -8.764e+05 | std 1.458e+06\n",
      "  Per-elem grad:    mean -2.853e+02 | std 4.747e+02\n",
      "  eps=1.0e-01 | total_err 1.521e+06 | per_elem_err 4.953e+02 | rel_err 9.971e-01\n",
      "  eps=3.0e-02 | total_err 1.507e+06 | per_elem_err 4.905e+02 | rel_err 9.880e-01\n",
      "  eps=1.0e-02 | total_err 1.513e+06 | per_elem_err 4.926e+02 | rel_err 9.915e-01\n",
      "  eps=3.0e-03 | total_err 1.492e+06 | per_elem_err 4.855e+02 | rel_err 9.833e-01\n",
      "  eps=1.0e-03 | total_err 1.398e+06 | per_elem_err 4.549e+02 | rel_err 9.129e-01\n",
      "\n",
      "[CIFAR (3x32x32)] Differentiability in input: autograd vs finite-difference\n",
      "  Input shape: torch.Size([4, 3, 32, 32])\n",
      "  Total dimensions: 3,072\n",
      "  Grad stats: mean 5.105e-01 | std 4.129e-01\n",
      "  Grad magnitude: mean |∇x| 5.522e-01\n",
      "  eps=1.0e-03 | abs_err 1.935e-05 | rel_err 8.885e-06\n",
      "  eps=3.0e-04 | abs_err 5.805e-06 | rel_err 2.665e-06\n",
      "  eps=1.0e-04 | abs_err 1.939e-06 | rel_err 8.902e-07\n",
      "  eps=3.0e-05 | abs_err 5.674e-07 | rel_err 2.605e-07\n",
      "  eps=1.0e-05 | abs_err 1.657e-07 | rel_err 7.606e-08\n",
      "\n",
      "=== IMAGENET-LIKE IMAGE FLOW ===\n",
      "Params: 26,128,932 (209.03 MB)\n",
      "\n",
      "[ImageNet (3x128x128)] Invertibility check\n",
      "  ||x_rec - x||_2    : mean 5.218e-13 | max 5.490e-13\n",
      "  ||x_rec - x||_inf  : mean 1.636e-14 | max 1.815e-14\n",
      "  rel L2 error       : mean 2.351e-15 | max 2.477e-15\n",
      "  |logdet + logdet^-1|: mean 1.819e-12 | max 3.638e-12\n",
      "  PASS: True\n",
      "\n",
      "[ImageNet (3x128x128)] Differentiability in time: autograd vs finite-difference\n",
      "  Dimension: 49,152\n",
      "  Total grad stats: mean -9.764e+06 | std 1.003e+07\n",
      "  Per-elem grad:    mean -1.986e+02 | std 2.040e+02\n",
      "  eps=1.0e-01 | total_err 1.237e+07 | per_elem_err 2.516e+02 | rel_err 1.011e+00\n",
      "  eps=3.0e-02 | total_err 1.236e+07 | per_elem_err 2.514e+02 | rel_err 1.009e+00\n",
      "  eps=1.0e-02 | total_err 1.286e+07 | per_elem_err 2.617e+02 | rel_err 1.018e+00\n",
      "  eps=3.0e-03 | total_err 1.395e+07 | per_elem_err 2.837e+02 | rel_err 1.083e+00\n",
      "  eps=1.0e-03 | total_err 1.556e+07 | per_elem_err 3.166e+02 | rel_err 1.138e+00\n",
      "\n",
      "[ImageNet (3x128x128)] Differentiability in input: autograd vs finite-difference\n",
      "  Input shape: torch.Size([4, 3, 128, 128])\n",
      "  Total dimensions: 49,152\n",
      "  Grad stats: mean 8.169e-01 | std 3.538e-01\n",
      "  Grad magnitude: mean |∇x| 8.172e-01\n",
      "  eps=1.0e-03 | abs_err 3.122e-05 | rel_err 4.924e-05\n",
      "  eps=3.0e-04 | abs_err 9.380e-06 | rel_err 1.480e-05\n",
      "  eps=1.0e-04 | abs_err 3.196e-06 | rel_err 5.041e-06\n",
      "  eps=3.0e-05 | abs_err 1.231e-06 | rel_err 1.942e-06\n",
      "  eps=1.0e-05 | abs_err 1.231e-06 | rel_err 1.942e-06\n",
      "\n",
      "=== FULL IMAGENET FLOW ===\n",
      "Params: 5,830,168 (46.64 MB)\n",
      "\n",
      "[ImageNet (3x224x224)] Invertibility check\n",
      "  ||x_rec - x||_2    : mean 5.292e-13 | max 5.528e-13\n",
      "  ||x_rec - x||_inf  : mean 8.896e-15 | max 9.326e-15\n",
      "  rel L2 error       : mean 1.364e-15 | max 1.427e-15\n",
      "  |logdet + logdet^-1|: mean 2.425e-12 | max 7.276e-12\n",
      "  PASS: True\n",
      "\n",
      "[ImageNet (3x224x224)] Differentiability in time: autograd vs finite-difference\n",
      "  Dimension: 150,528\n",
      "  Total grad stats: mean -5.193e+06 | std 2.549e+07\n",
      "  Per-elem grad:    mean -3.450e+01 | std 1.693e+02\n",
      "  eps=1.0e-01 | total_err 2.042e+07 | per_elem_err 1.356e+02 | rel_err 1.005e+00\n",
      "  eps=3.0e-02 | total_err 2.040e+07 | per_elem_err 1.355e+02 | rel_err 1.003e+00\n",
      "  eps=1.0e-02 | total_err 1.983e+07 | per_elem_err 1.317e+02 | rel_err 9.795e-01\n",
      "  eps=3.0e-03 | total_err 1.662e+07 | per_elem_err 1.104e+02 | rel_err 7.899e-01\n",
      "  eps=1.0e-03 | total_err 2.980e+07 | per_elem_err 1.980e+02 | rel_err 1.540e+00\n",
      "\n",
      "[ImageNet (3x224x224)] Differentiability in input: autograd vs finite-difference\n",
      "  Input shape: torch.Size([6, 3, 224, 224])\n",
      "  Total dimensions: 150,528\n",
      "  Grad stats: mean 7.962e-01 | std 4.242e-01\n",
      "  Grad magnitude: mean |∇x| 8.008e-01\n",
      "  eps=1.0e-03 | abs_err 2.527e-05 | rel_err 3.901e-05\n",
      "  eps=3.0e-04 | abs_err 7.614e-06 | rel_err 1.176e-05\n",
      "  eps=1.0e-04 | abs_err 2.376e-06 | rel_err 3.668e-06\n",
      "  eps=3.0e-05 | abs_err 2.413e-07 | rel_err 3.726e-07\n",
      "  eps=1.0e-05 | abs_err 2.437e-07 | rel_err 3.763e-07\n",
      "\n",
      "All checks done.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "quick_run_vector(device)\n",
    "torch.cuda.empty_cache(); gc.collect()  # Cleanup\n",
    "\n",
    "quick_run_mnist_like(device)\n",
    "torch.cuda.empty_cache(); gc.collect()  # Cleanup\n",
    "\n",
    "quick_run_cifar(device)\n",
    "torch.cuda.empty_cache(); gc.collect()  # Cleanup\n",
    "\n",
    "quick_run_imagenet(device)\n",
    "torch.cuda.empty_cache(); gc.collect()  # Cleanup\n",
    "\n",
    "quick_run_imagenet_full(device)  # Now there's room!\n",
    "torch.cuda.empty_cache(); gc.collect()\n",
    "print(\"\\nAll checks done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (OT_IMM)",
   "language": "python",
   "name": "ot_imm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
