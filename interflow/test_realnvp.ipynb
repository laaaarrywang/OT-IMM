{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Time-Indexed RealNVP Implementation\n",
    "\n",
    "This notebook comprehensively tests the time-indexed RealNVP flow T(t, x) for:\n",
    "1. **Invertibility**: T^{-1}(t, T(t, x)) = x\n",
    "2. **Differentiability w.r.t. time**: ∂T/∂t exists and is smooth\n",
    "3. **Differentiability w.r.t. input**: ∂T/∂x exists with correct Jacobian\n",
    "4. **Log-determinant consistency**: log|det(∂T/∂x)| matches forward and inverse\n",
    "5. **Multiple scales**: From tiny 8×8 to ImageNet 224×224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "================================================================================\n",
      "GPUs available: 1\n",
      "GPU 0: NVIDIA L40S, 44.4 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from realnvp import TimeIndexedRealNVP, create_vector_flow, create_mnist_flow,create_cifar10_flow, create_imagenet_flow, create_imagenet_flow_stable\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"GPUs available:\", torch.cuda.device_count())\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    props = torch.cuda.get_device_properties(i)\n",
    "    print(f\"GPU {i}: {props.name}, {props.total_memory/1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Count Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    n = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    mb = n * 8 / 1e6  # float32\n",
    "    return n, mb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check Invertibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def check_invertibility(model, x, t, name, atol=1e-5, rtol=1e-5):\n",
    "    model.eval()\n",
    "    y, ld1 = model(x, t)\n",
    "    x_rec, ld2 = model.inverse(y, t)\n",
    "\n",
    "    diff = (x_rec - x).reshape(x.shape[0], -1)\n",
    "    l2 = diff.norm(dim=1)\n",
    "    linf = diff.abs().max(dim=1).values\n",
    "    rel = l2 / (x.reshape(x.shape[0], -1).norm(dim=1) + 1e-12)\n",
    "\n",
    "    # logdet from inverse should be the negative of forward\n",
    "    logdet_consistency = (ld1 + ld2).abs()\n",
    "\n",
    "    print(f\"\\n[{name}] Invertibility check\")\n",
    "    print(f\"  ||x_rec - x||_2    : mean {l2.mean():.3e} | max {l2.max():.3e}\")\n",
    "    print(f\"  ||x_rec - x||_inf  : mean {linf.mean():.3e} | max {linf.max():.3e}\")\n",
    "    print(f\"  rel L2 error       : mean {rel.mean():.3e} | max {rel.max():.3e}\")\n",
    "    print(f\"  |logdet + logdet^-1|: mean {logdet_consistency.mean():.3e} | max {logdet_consistency.max():.3e}\")\n",
    "\n",
    "    ok = (l2.max() < atol + rtol * x.abs().max()) and (logdet_consistency.max() < 1e-8)    \n",
    "    print(f\"  PASS: {ok}\")\n",
    "    return ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check Differentiability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_differentiability_t(model, x, name, eps_list=(1e-1, 3e-2, 1e-2, 3e-3, 1e-3)):\n",
    "    model.eval()\n",
    "    B = x.shape[0]\n",
    "    device = x.device\n",
    "    D = x.view(B, -1).shape[1]  # Total dimension\n",
    "    \n",
    "    # random t in [0,1]\n",
    "    t = torch.rand(B, device=device, dtype=x.dtype, requires_grad=True)\n",
    "\n",
    "    # Autograd gradient of sum(y_i) w.r.t. t_i (per-sample)\n",
    "    y, _ = model(x, t)\n",
    "    f = y.view(B, -1).sum(dim=1)          # [B]\n",
    "    g_aut, = torch.autograd.grad(f.sum(), t, create_graph=False)  # [B]\n",
    "    \n",
    "    # Per-element gradients (normalized by dimension)\n",
    "    g_aut_per_elem = g_aut / D\n",
    "\n",
    "    print(f\"\\n[{name}] Differentiability in time: autograd vs finite-difference\")\n",
    "    print(f\"  Dimension: {D:,}\")\n",
    "    print(f\"  Total grad stats: mean {g_aut.mean().item():.3e} | std {g_aut.std().item():.3e}\")\n",
    "    print(f\"  Per-elem grad:    mean {g_aut_per_elem.mean().item():.3e} | std {g_aut_per_elem.std().item():.3e}\")\n",
    "\n",
    "    # Finite differences\n",
    "    with torch.no_grad():\n",
    "        for eps in eps_list:\n",
    "            t_plus = (t.detach() + eps).clamp(0.0, 1.0)\n",
    "            y_plus, _ = model(x, t_plus)\n",
    "            fd = (y_plus - y.detach()) / eps        # same shape as y\n",
    "            fd_sum = fd.view(B, -1).sum(dim=1)      # [B]\n",
    "\n",
    "            abs_err = (fd_sum - g_aut.detach()).abs()\n",
    "            rel_err = abs_err / (g_aut.detach().abs() + 1e-12)\n",
    "            \n",
    "            # Per-element errors\n",
    "            abs_err_per_elem = abs_err / D\n",
    "            rel_err_per_elem = rel_err  # relative error doesn't change with normalization\n",
    "\n",
    "            print(f\"  eps={eps:>7.1e} | total_err {abs_err.mean():.3e} | \"\n",
    "                  f\"per_elem_err {abs_err_per_elem.mean():.3e} | rel_err {rel_err_per_elem.mean():.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset-specific runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_run_imagenet(device=\"cpu\"):\n",
    "    print(\"\\n=== IMAGENET-LIKE IMAGE FLOW ===\")\n",
    "    model = create_imagenet_flow(\n",
    "        resolution=128,  # Smaller than default 224 for memory efficiency\n",
    "        num_layers=6, time_embed_dim=512,\n",
    "        img_base_channels=256, img_blocks=3, img_groups=32,\n",
    "        img_log_scale_clamp=10.0, use_permutation=True\n",
    "    ).to(device)\n",
    "    nparams, mb = count_params(model)\n",
    "    print(f\"Params: {nparams:,} ({mb:.2f} MB)\")\n",
    "    x = torch.randn(4, 3, 128, 128, device=device, dtype=torch.float64)  # Very small batch for memory\n",
    "    t = torch.rand(4, device=device, dtype=torch.float64)\n",
    "    check_invertibility(model, x, t, \"ImageNet (3x128x128)\")\n",
    "    check_differentiability_t(model, x, \"ImageNet (3x128x128)\")\n",
    "\n",
    "def quick_run_imagenet_full(device=\"cpu\"):\n",
    "    print(\"\\n=== FULL IMAGENET FLOW ===\")\n",
    "    model = create_imagenet_flow(\n",
    "        resolution=224,  # Full resolution\n",
    "        num_layers=4, time_embed_dim=256,\n",
    "        img_base_channels=128, img_blocks=4, img_groups=32,\n",
    "        img_log_scale_clamp=10.0, use_permutation=True\n",
    "    ).to(device)\n",
    "    nparams, mb = count_params(model)\n",
    "    print(f\"Params: {nparams:,} ({mb:.2f} MB)\")\n",
    "    x = torch.randn(6, 3, 224, 224, device=device, dtype=torch.float64)\n",
    "    t = torch.rand(6, device=device, dtype=torch.float64)\n",
    "    check_invertibility(model, x, t, \"ImageNet (3x224x224)\")\n",
    "    check_differentiability_t(model, x, \"ImageNet (3x224x224)\")\n",
    "\n",
    "def quick_run_vector(device=\"cpu\"):\n",
    "    print(\"\\n=== VECTOR FLOW ===\")\n",
    "    model = create_vector_flow(\n",
    "        dim=32, num_layers=6, time_embed_dim=64,\n",
    "        hidden=512, mlp_blocks=3, activation=\"gelu\",\n",
    "        use_layernorm=False, use_permutation=True\n",
    "    ).to(device)\n",
    "    nparams, mb = count_params(model)\n",
    "    print(f\"Params: {nparams:,} ({mb:.2f} MB)\")\n",
    "    x = torch.randn(8, 32, device=device, dtype=torch.float64)\n",
    "    t = torch.rand(8, device=device, dtype=torch.float64)\n",
    "    check_invertibility(model, x, t, \"vector(32)\")\n",
    "    check_differentiability_t(model, x, \"vector(32)\")\n",
    "\n",
    "def quick_run_mnist_like(device=\"cpu\"):\n",
    "    print(\"\\n=== MNIST-LIKE IMAGE FLOW ===\")\n",
    "    model = create_mnist_flow(\n",
    "        image_mode=True, num_layers=6, time_embed_dim=128,\n",
    "        img_base_channels=96, img_blocks=3, img_groups=32,\n",
    "        img_log_scale_clamp=10.0, use_permutation=True\n",
    "    ).to(device)\n",
    "    nparams, mb = count_params(model)\n",
    "    print(f\"Params: {nparams:,} ({mb:.2f} MB)\")\n",
    "    x = torch.randn(4, 1, 28, 28, device=device, dtype=torch.float64)\n",
    "    t = torch.rand(4, device=device, dtype=torch.float64)\n",
    "    check_invertibility(model, x, t, \"MNIST (1x28x28)\")\n",
    "    check_differentiability_t(model, x, \"MNIST (1x28x28)\")\n",
    "\n",
    "def quick_run_cifar(device=\"cpu\"):\n",
    "    print(\"\\n=== CIFAR-10 IMAGE FLOW ===\")\n",
    "    model = create_cifar10_flow(\n",
    "        num_layers=8, time_embed_dim=128,\n",
    "        img_base_channels=128, img_blocks=4, img_groups=32,\n",
    "        img_log_scale_clamp=10.0, use_permutation=True\n",
    "    ).to(device)\n",
    "    nparams, mb = count_params(model)\n",
    "    print(f\"Params: {nparams:,} ({mb:.2f} MB)\")\n",
    "    x = torch.randn(4, 3, 32, 32, device=device, dtype=torch.float64)  # keep small batch for speed\n",
    "    t = torch.rand(4, device=device, dtype=torch.float64)\n",
    "    check_invertibility(model, x, t, \"CIFAR (3x32x32)\")\n",
    "    check_differentiability_t(model, x, \"CIFAR (3x32x32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "=== VECTOR FLOW ===\n",
      "Params: 3,499,200 (27.99 MB)\n",
      "\n",
      "[vector(32)] Invertibility check\n",
      "  ||x_rec - x||_2    : mean 5.122e-16 | max 6.610e-16\n",
      "  ||x_rec - x||_inf  : mean 3.608e-16 | max 4.441e-16\n",
      "  rel L2 error       : mean 9.069e-17 | max 1.239e-16\n",
      "  |logdet + logdet^-1|: mean 8.882e-16 | max 1.776e-15\n",
      "  PASS: True\n",
      "\n",
      "[vector(32)] Differentiability in time: autograd vs finite-difference\n",
      "  Dimension: 32\n",
      "  Total grad stats: mean 8.828e-01 | std 8.072e+01\n",
      "  Per-elem grad:    mean 2.759e-02 | std 2.523e+00\n",
      "  eps=1.0e-01 | total_err 6.968e+01 | per_elem_err 2.178e+00 | rel_err 1.006e+00\n",
      "  eps=3.0e-02 | total_err 6.906e+01 | per_elem_err 2.158e+00 | rel_err 9.993e-01\n",
      "  eps=1.0e-02 | total_err 6.932e+01 | per_elem_err 2.166e+00 | rel_err 9.997e-01\n",
      "  eps=3.0e-03 | total_err 6.483e+01 | per_elem_err 2.026e+00 | rel_err 8.424e-01\n",
      "  eps=1.0e-03 | total_err 7.275e+01 | per_elem_err 2.273e+00 | rel_err 1.008e+00\n",
      "\n",
      "=== MNIST-LIKE IMAGE FLOW ===\n",
      "Params: 3,463,500 (27.71 MB)\n",
      "\n",
      "[MNIST (1x28x28)] Invertibility check\n",
      "  ||x_rec - x||_2    : mean 3.269e-14 | max 3.569e-14\n",
      "  ||x_rec - x||_inf  : mean 5.669e-15 | max 6.745e-15\n",
      "  rel L2 error       : mean 1.183e-15 | max 1.296e-15\n",
      "  |logdet + logdet^-1|: mean 3.553e-14 | max 5.684e-14\n",
      "  PASS: True\n",
      "\n",
      "[MNIST (1x28x28)] Differentiability in time: autograd vs finite-difference\n",
      "  Dimension: 784\n",
      "  Total grad stats: mean -2.887e+04 | std 4.068e+05\n",
      "  Per-elem grad:    mean -3.682e+01 | std 5.189e+02\n",
      "  eps=1.0e-01 | total_err 2.796e+05 | per_elem_err 3.566e+02 | rel_err 1.259e+00\n",
      "  eps=3.0e-02 | total_err 2.790e+05 | per_elem_err 3.558e+02 | rel_err 1.150e+00\n",
      "  eps=1.0e-02 | total_err 2.892e+05 | per_elem_err 3.689e+02 | rel_err 1.890e+00\n",
      "  eps=3.0e-03 | total_err 3.521e+05 | per_elem_err 4.491e+02 | rel_err 9.919e+00\n",
      "  eps=1.0e-03 | total_err 3.655e+05 | per_elem_err 4.662e+02 | rel_err 1.111e+01\n",
      "\n",
      "=== CIFAR-10 IMAGE FLOW ===\n",
      "Params: 10,611,760 (84.89 MB)\n",
      "\n",
      "[CIFAR (3x32x32)] Invertibility check\n",
      "  ||x_rec - x||_2    : mean 1.691e-13 | max 1.812e-13\n",
      "  ||x_rec - x||_inf  : mean 1.393e-14 | max 1.488e-14\n",
      "  rel L2 error       : mean 3.071e-15 | max 3.274e-15\n",
      "  |logdet + logdet^-1|: mean 1.137e-13 | max 4.547e-13\n",
      "  PASS: True\n",
      "\n",
      "[CIFAR (3x32x32)] Differentiability in time: autograd vs finite-difference\n",
      "  Dimension: 3,072\n",
      "  Total grad stats: mean -3.498e+05 | std 1.361e+06\n",
      "  Per-elem grad:    mean -1.139e+02 | std 4.431e+02\n",
      "  eps=1.0e-01 | total_err 8.979e+05 | per_elem_err 2.923e+02 | rel_err 9.850e-01\n",
      "  eps=3.0e-02 | total_err 9.028e+05 | per_elem_err 2.939e+02 | rel_err 1.056e+00\n",
      "  eps=1.0e-02 | total_err 8.785e+05 | per_elem_err 2.860e+02 | rel_err 8.255e-01\n",
      "  eps=3.0e-03 | total_err 8.338e+05 | per_elem_err 2.714e+02 | rel_err 7.494e-01\n",
      "  eps=1.0e-03 | total_err 1.086e+06 | per_elem_err 3.535e+02 | rel_err 1.586e+00\n",
      "\n",
      "=== IMAGENET-LIKE IMAGE FLOW ===\n",
      "Params: 26,128,932 (209.03 MB)\n",
      "\n",
      "[ImageNet (3x128x128)] Invertibility check\n",
      "  ||x_rec - x||_2    : mean 4.969e-13 | max 5.217e-13\n",
      "  ||x_rec - x||_inf  : mean 1.460e-14 | max 1.599e-14\n",
      "  rel L2 error       : mean 2.240e-15 | max 2.353e-15\n",
      "  |logdet + logdet^-1|: mean 2.728e-12 | max 3.638e-12\n",
      "  PASS: True\n",
      "\n",
      "[ImageNet (3x128x128)] Differentiability in time: autograd vs finite-difference\n",
      "  Dimension: 49,152\n",
      "  Total grad stats: mean -8.791e+06 | std 2.671e+07\n",
      "  Per-elem grad:    mean -1.789e+02 | std 5.435e+02\n",
      "  eps=1.0e-01 | total_err 2.156e+07 | per_elem_err 4.386e+02 | rel_err 1.031e+00\n",
      "  eps=3.0e-02 | total_err 2.149e+07 | per_elem_err 4.373e+02 | rel_err 1.102e+00\n",
      "  eps=1.0e-02 | total_err 2.143e+07 | per_elem_err 4.360e+02 | rel_err 1.234e+00\n",
      "  eps=3.0e-03 | total_err 2.378e+07 | per_elem_err 4.838e+02 | rel_err 1.716e+00\n",
      "  eps=1.0e-03 | total_err 2.148e+07 | per_elem_err 4.371e+02 | rel_err 2.502e+00\n",
      "\n",
      "=== FULL IMAGENET FLOW ===\n",
      "Params: 5,830,168 (46.64 MB)\n",
      "\n",
      "[ImageNet (3x224x224)] Invertibility check\n",
      "  ||x_rec - x||_2    : mean 5.838e-13 | max 6.166e-13\n",
      "  ||x_rec - x||_inf  : mean 1.002e-14 | max 1.155e-14\n",
      "  rel L2 error       : mean 1.504e-15 | max 1.591e-15\n",
      "  |logdet + logdet^-1|: mean 2.425e-12 | max 7.276e-12\n",
      "  PASS: True\n",
      "\n",
      "[ImageNet (3x224x224)] Differentiability in time: autograd vs finite-difference\n",
      "  Dimension: 150,528\n",
      "  Total grad stats: mean 2.170e+07 | std 7.454e+07\n",
      "  Per-elem grad:    mean 1.442e+02 | std 4.952e+02\n",
      "  eps=1.0e-01 | total_err 4.994e+07 | per_elem_err 3.317e+02 | rel_err 1.001e+00\n",
      "  eps=3.0e-02 | total_err 4.985e+07 | per_elem_err 3.312e+02 | rel_err 1.048e+00\n",
      "  eps=1.0e-02 | total_err 4.915e+07 | per_elem_err 3.265e+02 | rel_err 9.069e-01\n",
      "  eps=3.0e-03 | total_err 5.008e+07 | per_elem_err 3.327e+02 | rel_err 1.422e+00\n",
      "  eps=1.0e-03 | total_err 5.012e+07 | per_elem_err 3.330e+02 | rel_err 1.581e+00\n",
      "\n",
      "All checks done.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "quick_run_vector(device)\n",
    "torch.cuda.empty_cache(); gc.collect()  # Cleanup\n",
    "\n",
    "quick_run_mnist_like(device)\n",
    "torch.cuda.empty_cache(); gc.collect()  # Cleanup\n",
    "\n",
    "quick_run_cifar(device)\n",
    "torch.cuda.empty_cache(); gc.collect()  # Cleanup\n",
    "\n",
    "quick_run_imagenet(device)\n",
    "torch.cuda.empty_cache(); gc.collect()  # Cleanup\n",
    "\n",
    "quick_run_imagenet_full(device)  # Now there's room!\n",
    "torch.cuda.empty_cache(); gc.collect()\n",
    "print(\"\\nAll checks done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (OT_IMM)",
   "language": "python",
   "name": "ot_imm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
