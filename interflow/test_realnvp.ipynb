{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Time-Indexed RealNVP Implementation\n",
    "\n",
    "This notebook comprehensively tests the time-indexed RealNVP flow T(t, x) for:\n",
    "1. **Invertibility**: T^{-1}(t, T(t, x)) = x\n",
    "2. **Differentiability w.r.t. time**: ∂T/∂t exists and is smooth\n",
    "3. **Differentiability w.r.t. input**: ∂T/∂x exists with correct Jacobian\n",
    "4. **Log-determinant consistency**: log|det(∂T/∂x)| matches forward and inverse\n",
    "5. **Multiple scales**: From tiny 8×8 to ImageNet 224×224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "================================================================================\n",
      "GPUs available: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from realnvp import TimeIndexedRealNVP, create_vector_flow, create_mnist_flow,create_cifar10_flow, create_imagenet_flow, create_imagenet_flow_stable\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"GPUs available:\", torch.cuda.device_count())\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    props = torch.cuda.get_device_properties(i)\n",
    "    print(f\"GPU {i}: {props.name}, {props.total_memory/1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Count Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    n = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    mb = n * 8 / 1e6  # float32\n",
    "    return n, mb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check Invertibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def check_invertibility(model, x, t, name, atol=1e-5, rtol=1e-5):\n",
    "    model.eval()\n",
    "    y, ld1 = model(x, t)\n",
    "    x_rec, ld2 = model.inverse(y, t)\n",
    "\n",
    "    diff = (x_rec - x).reshape(x.shape[0], -1)\n",
    "    l2 = diff.norm(dim=1)\n",
    "    linf = diff.abs().max(dim=1).values\n",
    "    rel = l2 / (x.reshape(x.shape[0], -1).norm(dim=1) + 1e-12)\n",
    "\n",
    "    # logdet from inverse should be the negative of forward\n",
    "    logdet_consistency = (ld1 + ld2).abs()\n",
    "\n",
    "    print(f\"\\n[{name}] Invertibility check\")\n",
    "    print(f\"  ||x_rec - x||_2    : mean {l2.mean():.3e} | max {l2.max():.3e}\")\n",
    "    print(f\"  ||x_rec - x||_inf  : mean {linf.mean():.3e} | max {linf.max():.3e}\")\n",
    "    print(f\"  rel L2 error       : mean {rel.mean():.3e} | max {rel.max():.3e}\")\n",
    "    print(f\"  |logdet + logdet^-1|: mean {logdet_consistency.mean():.3e} | max {logdet_consistency.max():.3e}\")\n",
    "\n",
    "    ok = (l2.max() < atol + rtol * x.abs().max()) and (logdet_consistency.max() < 1e-8)    \n",
    "    print(f\"  PASS: {ok}\")\n",
    "    return ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check Differentiability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_differentiability_t(model, x, name, eps_list=(1e-1, 3e-2, 1e-2, 3e-3, 1e-3)):\n",
    "    \"\"\"Check differentiability w.r.t. time using autograd vs finite differences\"\"\"\n",
    "    model.eval()\n",
    "    B = x.shape[0]\n",
    "    device = x.device\n",
    "    D = x.view(B, -1).shape[1]  # Total dimension\n",
    "    \n",
    "    # random t in [0,1]\n",
    "    t = torch.rand(B, device=device, dtype=x.dtype, requires_grad=True)\n",
    "\n",
    "    # Autograd gradient of sum(y_i) w.r.t. t_i (per-sample)\n",
    "    y, _ = model(x, t)\n",
    "    f = y.view(B, -1).sum(dim=1)          # [B]\n",
    "    g_aut, = torch.autograd.grad(f.sum(), t, create_graph=False)  # [B]\n",
    "    \n",
    "    # Per-element gradients (normalized by dimension)\n",
    "    g_aut_per_elem = g_aut / D\n",
    "\n",
    "    print(f\"\\n[{name}] Differentiability in time: autograd vs finite-difference\")\n",
    "    print(f\"  Dimension: {D:,}\")\n",
    "    print(f\"  Total grad stats: mean {g_aut.mean().item():.3e} | std {g_aut.std().item():.3e}\")\n",
    "    print(f\"  Per-elem grad:    mean {g_aut_per_elem.mean().item():.3e} | std {g_aut_per_elem.std().item():.3e}\")\n",
    "\n",
    "    # Finite differences\n",
    "    with torch.no_grad():\n",
    "        for eps in eps_list:\n",
    "            t_plus = (t.detach() + eps).clamp(0.0, 1.0)\n",
    "            y_plus, _ = model(x, t_plus)\n",
    "            fd = (y_plus - y.detach()) / eps        # same shape as y\n",
    "            fd_sum = fd.view(B, -1).sum(dim=1)      # [B]\n",
    "\n",
    "            abs_err = (fd_sum - g_aut.detach()).abs()\n",
    "            rel_err = abs_err / (g_aut.detach().abs() + 1e-12)\n",
    "            \n",
    "            # Per-element errors\n",
    "            abs_err_per_elem = abs_err / D\n",
    "            rel_err_per_elem = rel_err  # relative error doesn't change with normalization\n",
    "\n",
    "            print(f\"  eps={eps:>7.1e} | total_err {abs_err.mean():.3e} | \"\n",
    "                  f\"per_elem_err {abs_err_per_elem.mean():.3e} | rel_err {rel_err_per_elem.mean():.3e}\")\n",
    "\n",
    "def check_differentiability_x(model, x_template, name, eps_list=(1e-3, 3e-4, 1e-4, 3e-5, 1e-5)):\n",
    "    \"\"\"Check differentiability w.r.t. input x using autograd vs finite differences\"\"\"\n",
    "    model.eval()\n",
    "    B = x_template.shape[0]\n",
    "    device = x_template.device\n",
    "    D = x_template.view(B, -1).shape[1]  # Total dimension\n",
    "    \n",
    "    # Create input x that requires grad\n",
    "    x = x_template.clone().detach().requires_grad_(True)\n",
    "    t = torch.rand(B, device=device, dtype=x.dtype)  # Fixed time for this test\n",
    "    \n",
    "    # Forward pass\n",
    "    y, logdet = model(x, t)\n",
    "    \n",
    "    # Compute scalar output for differentiation (sum of all outputs)\n",
    "    f = y.view(B, -1).sum(dim=1).sum()  # Single scalar\n",
    "    \n",
    "    # Autograd gradient w.r.t. x\n",
    "    g_aut = torch.autograd.grad(f, x, create_graph=False)[0]  # Same shape as x\n",
    "    g_aut_flat = g_aut.view(B, -1)  # [B, D]\n",
    "    \n",
    "    # Per-element gradient statistics\n",
    "    g_aut_per_elem = g_aut_flat.abs().mean()\n",
    "\n",
    "    print(f\"\\n[{name}] Differentiability in input: autograd vs finite-difference\")\n",
    "    print(f\"  Input shape: {x.shape}\")\n",
    "    print(f\"  Total dimensions: {D:,}\")\n",
    "    print(f\"  Grad stats: mean {g_aut_flat.mean().item():.3e} | std {g_aut_flat.std().item():.3e}\")\n",
    "    print(f\"  Grad magnitude: mean |∇x| {g_aut_per_elem.item():.3e}\")\n",
    "\n",
    "    # Finite differences - test a few random directions\n",
    "    with torch.no_grad():\n",
    "        # Create random perturbation directions (normalized)\n",
    "        directions = torch.randn_like(x)\n",
    "        directions = directions / directions.view(B, -1).norm(dim=1, keepdim=True).view(B, *([1] * (x.dim()-1)))\n",
    "        \n",
    "        for eps in eps_list:\n",
    "            # Perturb input\n",
    "            x_plus = x.detach() + eps * directions\n",
    "            \n",
    "            # Forward pass with perturbed input\n",
    "            y_plus, logdet_plus = model(x_plus, t)\n",
    "            f_plus = y_plus.view(B, -1).sum(dim=1).sum()\n",
    "            \n",
    "            # Finite difference approximation\n",
    "            fd_scalar = (f_plus - f.detach()) / eps\n",
    "            \n",
    "            # Directional derivative from autograd: ∇f · direction\n",
    "            directional_grad = (g_aut.detach() * directions).view(B, -1).sum(dim=1).sum()\n",
    "            \n",
    "            # Compare\n",
    "            abs_err = abs(fd_scalar - directional_grad)\n",
    "            rel_err = abs_err / (abs(directional_grad) + 1e-12)\n",
    "            \n",
    "            print(f\"  eps={eps:>7.1e} | abs_err {abs_err:.3e} | rel_err {rel_err:.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset-specific runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_run_imagenet(device=\"cpu\"):\n",
    "    print(\"\\n=== IMAGENET-LIKE IMAGE FLOW ===\")\n",
    "    model = create_imagenet_flow(\n",
    "        resolution=128,  # Smaller than default 224 for memory efficiency\n",
    "        num_layers=6, time_embed_dim=512,\n",
    "        img_base_channels=256, img_blocks=3, img_groups=32,\n",
    "        img_log_scale_clamp=10.0, use_permutation=True\n",
    "    ).to(device)\n",
    "    nparams, mb = count_params(model)\n",
    "    print(f\"Params: {nparams:,} ({mb:.2f} MB)\")\n",
    "    x = torch.randn(4, 3, 128, 128, device=device, dtype=torch.float32)  # Very small batch for memory\n",
    "    t = torch.rand(4, device=device, dtype=torch.float32)\n",
    "    check_invertibility(model, x, t, \"ImageNet (3x128x128)\")\n",
    "    check_differentiability_t(model, x, \"ImageNet (3x128x128)\")\n",
    "    check_differentiability_x(model, x, \"ImageNet (3x128x128)\")\n",
    "\n",
    "def quick_run_imagenet_full(device=\"cpu\"):\n",
    "    print(\"\\n=== FULL IMAGENET FLOW ===\")\n",
    "    model = create_imagenet_flow(\n",
    "        resolution=224,  # Full resolution\n",
    "        num_layers=4, time_embed_dim=256,\n",
    "        img_base_channels=128, img_blocks=4, img_groups=32,\n",
    "        img_log_scale_clamp=10.0, use_permutation=True\n",
    "    ).to(device)\n",
    "    nparams, mb = count_params(model)\n",
    "    print(f\"Params: {nparams:,} ({mb:.2f} MB)\")\n",
    "    x = torch.randn(6, 3, 224, 224, device=device, dtype=torch.float32)\n",
    "    t = torch.rand(6, device=device, dtype=torch.float32)\n",
    "    check_invertibility(model, x, t, \"ImageNet (3x224x224)\")\n",
    "    check_differentiability_t(model, x, \"ImageNet (3x224x224)\")\n",
    "    check_differentiability_x(model, x, \"ImageNet (3x224x224)\")\n",
    "\n",
    "def quick_run_vector(device=\"cpu\"):\n",
    "    print(\"\\n=== VECTOR FLOW ===\")\n",
    "    model = create_vector_flow(\n",
    "        dim=32, num_layers=6, time_embed_dim=64,\n",
    "        hidden=512, mlp_blocks=3, activation=\"gelu\",\n",
    "        use_layernorm=False, use_permutation=True\n",
    "    ).to(device)\n",
    "    nparams, mb = count_params(model)\n",
    "    print(f\"Params: {nparams:,} ({mb:.2f} MB)\")\n",
    "    x = torch.randn(8, 32, device=device, dtype=torch.float32)\n",
    "    t = torch.rand(8, device=device, dtype=torch.float32)\n",
    "    check_invertibility(model, x, t, \"vector(32)\")\n",
    "    check_differentiability_t(model, x, \"vector(32)\")\n",
    "    check_differentiability_x(model, x, \"vector(32)\")\n",
    "\n",
    "def quick_run_mnist_like(device=\"cpu\"):\n",
    "    print(\"\\n=== MNIST-LIKE IMAGE FLOW ===\")\n",
    "    model = create_mnist_flow(\n",
    "        image_mode=True, num_layers=6, time_embed_dim=128,\n",
    "        img_base_channels=96, img_blocks=3, img_groups=32,\n",
    "        img_log_scale_clamp=10.0, use_permutation=True\n",
    "    ).to(device)\n",
    "    nparams, mb = count_params(model)\n",
    "    print(f\"Params: {nparams:,} ({mb:.2f} MB)\")\n",
    "    x = torch.randn(4, 1, 28, 28, device=device, dtype=torch.float32)\n",
    "    t = torch.rand(4, device=device, dtype=torch.float32)\n",
    "    check_invertibility(model, x, t, \"MNIST (1x28x28)\")\n",
    "    check_differentiability_t(model, x, \"MNIST (1x28x28)\")\n",
    "    check_differentiability_x(model, x, \"MNIST (1x28x28)\")\n",
    "\n",
    "def quick_run_cifar(device=\"cpu\"):\n",
    "    print(\"\\n=== CIFAR-10 IMAGE FLOW ===\")\n",
    "    model = create_cifar10_flow(\n",
    "        num_layers=8, time_embed_dim=128,\n",
    "        img_base_channels=128, img_blocks=4, img_groups=32,\n",
    "        img_log_scale_clamp=10.0, use_permutation=True\n",
    "    ).to(device)\n",
    "    nparams, mb = count_params(model)\n",
    "    print(f\"Params: {nparams:,} ({mb:.2f} MB)\")\n",
    "    x = torch.randn(4, 3, 32, 32, device=device, dtype=torch.float32)  # keep small batch for speed\n",
    "    t = torch.rand(4, device=device, dtype=torch.float32)\n",
    "    check_invertibility(model, x, t, \"CIFAR (3x32x32)\")\n",
    "    check_differentiability_t(model, x, \"CIFAR (3x32x32)\")\n",
    "    check_differentiability_x(model, x, \"CIFAR (3x32x32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "\n",
      "=== VECTOR FLOW ===\n",
      "Params: 3,499,200 (27.99 MB)\n",
      "\n",
      "[vector(32)] Invertibility check\n",
      "  ||x_rec - x||_2    : mean 2.484e-07 | max 4.971e-07\n",
      "  ||x_rec - x||_inf  : mean 1.788e-07 | max 4.768e-07\n",
      "  rel L2 error       : mean 4.306e-08 | max 7.834e-08\n",
      "  |logdet + logdet^-1|: mean 7.153e-07 | max 9.537e-07\n",
      "  PASS: False\n",
      "\n",
      "[vector(32)] Differentiability in time: autograd vs finite-difference\n",
      "  Dimension: 32\n",
      "  Total grad stats: mean 2.346e+00 | std 6.113e+01\n",
      "  Per-elem grad:    mean 7.331e-02 | std 1.910e+00\n",
      "  eps=1.0e-01 | total_err 4.944e+01 | per_elem_err 1.545e+00 | rel_err 1.008e+00\n",
      "  eps=3.0e-02 | total_err 4.999e+01 | per_elem_err 1.562e+00 | rel_err 1.012e+00\n",
      "  eps=1.0e-02 | total_err 5.002e+01 | per_elem_err 1.563e+00 | rel_err 1.017e+00\n",
      "  eps=3.0e-03 | total_err 4.987e+01 | per_elem_err 1.558e+00 | rel_err 1.018e+00\n",
      "  eps=1.0e-03 | total_err 6.285e+01 | per_elem_err 1.964e+00 | rel_err 1.251e+00\n",
      "\n",
      "[vector(32)] Differentiability in input: autograd vs finite-difference\n",
      "  Input shape: torch.Size([8, 32])\n",
      "  Total dimensions: 32\n",
      "  Grad stats: mean 6.951e-01 | std 1.127e-01\n",
      "  Grad magnitude: mean |∇x| 6.951e-01\n",
      "  eps=1.0e-03 | abs_err 9.159e-04 | rel_err 6.954e-04\n",
      "  eps=3.0e-04 | abs_err 8.863e-03 | rel_err 6.730e-03\n",
      "  eps=1.0e-04 | abs_err 1.999e-02 | rel_err 1.518e-02\n",
      "  eps=3.0e-05 | abs_err 7.721e-02 | rel_err 5.863e-02\n",
      "  eps=1.0e-05 | abs_err 7.721e-02 | rel_err 5.863e-02\n",
      "\n",
      "=== MNIST-LIKE IMAGE FLOW ===\n",
      "Params: 3,463,500 (27.71 MB)\n",
      "\n",
      "[MNIST (1x28x28)] Invertibility check\n",
      "  ||x_rec - x||_2    : mean 1.584e-05 | max 1.663e-05\n",
      "  ||x_rec - x||_inf  : mean 2.183e-06 | max 2.623e-06\n",
      "  rel L2 error       : mean 5.772e-07 | max 5.996e-07\n",
      "  |logdet + logdet^-1|: mean 1.526e-05 | max 3.052e-05\n",
      "  PASS: False\n",
      "\n",
      "[MNIST (1x28x28)] Differentiability in time: autograd vs finite-difference\n",
      "  Dimension: 784\n",
      "  Total grad stats: mean -1.823e+05 | std 4.516e+05\n",
      "  Per-elem grad:    mean -2.326e+02 | std 5.760e+02\n",
      "  eps=1.0e-01 | total_err 2.943e+05 | per_elem_err 3.754e+02 | rel_err 9.721e-01\n",
      "  eps=3.0e-02 | total_err 2.997e+05 | per_elem_err 3.823e+02 | rel_err 9.809e-01\n",
      "  eps=1.0e-02 | total_err 3.146e+05 | per_elem_err 4.012e+02 | rel_err 1.548e+00\n",
      "  eps=3.0e-03 | total_err 3.397e+05 | per_elem_err 4.334e+02 | rel_err 2.160e+00\n",
      "  eps=1.0e-03 | total_err 4.116e+05 | per_elem_err 5.251e+02 | rel_err 1.610e+00\n",
      "\n",
      "[MNIST (1x28x28)] Differentiability in input: autograd vs finite-difference\n",
      "  Input shape: torch.Size([4, 1, 28, 28])\n",
      "  Total dimensions: 784\n",
      "  Grad stats: mean 9.345e-01 | std 3.998e-01\n",
      "  Grad magnitude: mean |∇x| 9.349e-01\n",
      "  eps=1.0e-03 | abs_err 2.063e-01 | rel_err 5.506e-02\n",
      "  eps=3.0e-04 | abs_err 8.419e-02 | rel_err 2.247e-02\n",
      "  eps=1.0e-04 | abs_err 2.526e+00 | rel_err 6.742e-01\n",
      "  eps=3.0e-05 | abs_err 9.850e+00 | rel_err 2.629e+00\n",
      "  eps=1.0e-05 | abs_err 9.850e+00 | rel_err 2.629e+00\n",
      "\n",
      "=== CIFAR-10 IMAGE FLOW ===\n",
      "Params: 10,611,760 (84.89 MB)\n",
      "\n",
      "[CIFAR (3x32x32)] Invertibility check\n",
      "  ||x_rec - x||_2    : mean 7.073e-05 | max 7.231e-05\n",
      "  ||x_rec - x||_inf  : mean 6.610e-06 | max 7.427e-06\n",
      "  rel L2 error       : mean 1.278e-06 | max 1.321e-06\n",
      "  |logdet + logdet^-1|: mean 9.155e-05 | max 1.221e-04\n",
      "  PASS: False\n",
      "\n",
      "[CIFAR (3x32x32)] Differentiability in time: autograd vs finite-difference\n",
      "  Dimension: 3,072\n",
      "  Total grad stats: mean -8.913e+05 | std 4.270e+05\n",
      "  Per-elem grad:    mean -2.901e+02 | std 1.390e+02\n",
      "  eps=1.0e-01 | total_err 8.907e+05 | per_elem_err 2.899e+02 | rel_err 1.002e+00\n",
      "  eps=3.0e-02 | total_err 8.939e+05 | per_elem_err 2.910e+02 | rel_err 1.005e+00\n",
      "  eps=1.0e-02 | total_err 9.213e+05 | per_elem_err 2.999e+02 | rel_err 1.064e+00\n",
      "  eps=3.0e-03 | total_err 1.004e+06 | per_elem_err 3.269e+02 | rel_err 1.129e+00\n",
      "  eps=1.0e-03 | total_err 1.275e+06 | per_elem_err 4.151e+02 | rel_err 1.654e+00\n",
      "\n",
      "[CIFAR (3x32x32)] Differentiability in input: autograd vs finite-difference\n",
      "  Input shape: torch.Size([4, 3, 32, 32])\n",
      "  Total dimensions: 3,072\n",
      "  Grad stats: mean 6.466e-01 | std 3.800e-01\n",
      "  Grad magnitude: mean |∇x| 6.532e-01\n",
      "  eps=1.0e-03 | abs_err 1.745e-01 | rel_err 2.504e+00\n",
      "  eps=3.0e-04 | abs_err 4.766e-01 | rel_err 6.839e+00\n",
      "  eps=1.0e-04 | abs_err 2.511e+00 | rel_err 3.604e+01\n",
      "  eps=3.0e-05 | abs_err 3.999e+00 | rel_err 5.739e+01\n",
      "  eps=1.0e-05 | abs_err 2.448e+01 | rel_err 3.514e+02\n",
      "\n",
      "=== IMAGENET-LIKE IMAGE FLOW ===\n",
      "Params: 26,128,932 (209.03 MB)\n",
      "\n",
      "[ImageNet (3x128x128)] Invertibility check\n",
      "  ||x_rec - x||_2    : mean 1.925e-04 | max 1.985e-04\n",
      "  ||x_rec - x||_inf  : mean 5.584e-06 | max 6.199e-06\n",
      "  rel L2 error       : mean 8.696e-07 | max 8.983e-07\n",
      "  |logdet + logdet^-1|: mean 9.766e-04 | max 1.953e-03\n",
      "  PASS: False\n",
      "\n",
      "[ImageNet (3x128x128)] Differentiability in time: autograd vs finite-difference\n",
      "  Dimension: 49,152\n",
      "  Total grad stats: mean -2.401e+06 | std 8.614e+06\n",
      "  Per-elem grad:    mean -4.885e+01 | std 1.752e+02\n",
      "  eps=1.0e-01 | total_err 6.673e+06 | per_elem_err 1.358e+02 | rel_err 9.835e-01\n",
      "  eps=3.0e-02 | total_err 6.674e+06 | per_elem_err 1.358e+02 | rel_err 9.767e-01\n",
      "  eps=1.0e-02 | total_err 5.717e+06 | per_elem_err 1.163e+02 | rel_err 7.546e-01\n",
      "  eps=3.0e-03 | total_err 5.826e+06 | per_elem_err 1.185e+02 | rel_err 8.613e-01\n",
      "  eps=1.0e-03 | total_err 7.895e+06 | per_elem_err 1.606e+02 | rel_err 1.765e+00\n",
      "\n",
      "[ImageNet (3x128x128)] Differentiability in input: autograd vs finite-difference\n",
      "  Input shape: torch.Size([4, 3, 128, 128])\n",
      "  Total dimensions: 49,152\n",
      "  Grad stats: mean 6.482e-01 | std 3.179e-01\n",
      "  Grad magnitude: mean |∇x| 6.511e-01\n",
      "  eps=1.0e-03 | abs_err 7.949e-01 | rel_err 1.000e+00\n",
      "  eps=3.0e-04 | abs_err 2.525e+01 | rel_err 3.176e+01\n",
      "  eps=1.0e-04 | abs_err 7.733e+01 | rel_err 9.729e+01\n",
      "  eps=3.0e-05 | abs_err 7.949e-01 | rel_err 1.000e+00\n",
      "  eps=1.0e-05 | abs_err 7.805e+02 | rel_err 9.819e+02\n",
      "\n",
      "=== FULL IMAGENET FLOW ===\n",
      "Params: 5,830,168 (46.64 MB)\n",
      "\n",
      "[ImageNet (3x224x224)] Invertibility check\n",
      "  ||x_rec - x||_2    : mean 2.607e-04 | max 2.672e-04\n",
      "  ||x_rec - x||_inf  : mean 4.410e-06 | max 4.947e-06\n",
      "  rel L2 error       : mean 6.718e-07 | max 6.895e-07\n",
      "  |logdet + logdet^-1|: mean 0.000e+00 | max 0.000e+00\n",
      "  PASS: False\n",
      "\n",
      "[ImageNet (3x224x224)] Differentiability in time: autograd vs finite-difference\n",
      "  Dimension: 150,528\n",
      "  Total grad stats: mean 5.125e+06 | std 4.865e+07\n",
      "  Per-elem grad:    mean 3.405e+01 | std 3.232e+02\n",
      "  eps=1.0e-01 | total_err 3.449e+07 | per_elem_err 2.291e+02 | rel_err 9.862e-01\n",
      "  eps=3.0e-02 | total_err 3.462e+07 | per_elem_err 2.300e+02 | rel_err 7.946e+00\n",
      "  eps=1.0e-02 | total_err 3.405e+07 | per_elem_err 2.262e+02 | rel_err 7.647e+00\n",
      "  eps=3.0e-03 | total_err 3.495e+07 | per_elem_err 2.322e+02 | rel_err 3.242e+01\n",
      "  eps=1.0e-03 | total_err 3.345e+07 | per_elem_err 2.222e+02 | rel_err 1.773e+02\n",
      "\n",
      "[ImageNet (3x224x224)] Differentiability in input: autograd vs finite-difference\n",
      "  Input shape: torch.Size([6, 3, 224, 224])\n",
      "  Total dimensions: 150,528\n",
      "  Grad stats: mean 5.369e-01 | std 2.845e-01\n",
      "  Grad magnitude: mean |∇x| 5.494e-01\n",
      "  eps=1.0e-03 | abs_err 1.689e+01 | rel_err 1.334e+01\n",
      "  eps=3.0e-04 | abs_err 2.478e+01 | rel_err 1.957e+01\n",
      "  eps=1.0e-04 | abs_err 1.266e+00 | rel_err 1.000e+00\n",
      "  eps=3.0e-05 | abs_err 1.266e+00 | rel_err 1.000e+00\n",
      "  eps=1.0e-05 | abs_err 1.266e+00 | rel_err 1.000e+00\n",
      "\n",
      "All checks done.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "quick_run_vector(device)\n",
    "torch.cuda.empty_cache(); gc.collect()  # Cleanup\n",
    "\n",
    "quick_run_mnist_like(device)\n",
    "torch.cuda.empty_cache(); gc.collect()  # Cleanup\n",
    "\n",
    "quick_run_cifar(device)\n",
    "torch.cuda.empty_cache(); gc.collect()  # Cleanup\n",
    "\n",
    "quick_run_imagenet(device)\n",
    "torch.cuda.empty_cache(); gc.collect()  # Cleanup\n",
    "\n",
    "quick_run_imagenet_full(device)  # Now there's room!\n",
    "torch.cuda.empty_cache(); gc.collect()\n",
    "print(\"\\nAll checks done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb-fbsde-arm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
