{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear Stochastic Interpolant with Neural Spline Flows (Debugged)\n",
    "## Testing on Checker Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('../')\n",
    "from typing import Tuple, Any\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import interflow as itf\n",
    "import interflow.prior as prior\n",
    "import interflow.fabrics\n",
    "import interflow.stochastic_interpolant as stochastic_interpolant\n",
    "from interflow.nonlinear_stochastic_interpolant_v2 import (\n",
    "    NonlinearInterpolantV2, NonlinearSITrainerV2\n",
    ")\n",
    "\n",
    "# Check CUDA availability\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA available, setting default tensor residence to GPU.')\n",
    "    device = torch.device('cuda')\n",
    "    itf.util.set_torch_device('cuda')\n",
    "else:\n",
    "    print('No CUDA device found, using CPU.')\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab(var):\n",
    "    \"\"\"Take a tensor off the gpu and convert it to a numpy array on the CPU.\"\"\"\n",
    "    if isinstance(var, torch.Tensor):\n",
    "        return var.detach().cpu().numpy()\n",
    "    return var\n",
    "\n",
    "def compute_likelihoods(\n",
    "    b: torch.nn.Module,\n",
    "    s: torch.nn.Module,\n",
    "    interpolant: stochastic_interpolant.Interpolant,\n",
    "    n_save: int,\n",
    "    n_step: int,\n",
    "    eps: float,\n",
    "    bs: int,\n",
    "    base,\n",
    "    device\n",
    ") -> Tuple[np.ndarray, np.ndarray, torch.Tensor]:\n",
    "    \"\"\"Draw samples from the probability flow and SDE models, and compute likelihoods.\"\"\"\n",
    "    \n",
    "    sde_flow = stochastic_interpolant.SDEIntegrator(\n",
    "        b=b, s=s, eps=eps, interpolant=interpolant, n_save=n_save, n_likelihood=1, n_step=n_step\n",
    "    )\n",
    "    pflow = stochastic_interpolant.PFlowIntegrator(\n",
    "        b=b,  \n",
    "        method='dopri5', \n",
    "        interpolant=interpolant,\n",
    "        n_step=3\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x0_tests = base(bs).to(device)\n",
    "        xfs_sde = sde_flow.rollout_forward(x0_tests)\n",
    "        xf_sde = grab(xfs_sde[-1].squeeze())\n",
    "    \n",
    "    logp0 = base.log_prob(x0_tests)\n",
    "    xfs_pflow, dlogp_pflow = pflow.rollout(x0_tests)\n",
    "    logpx_pflow = logp0 + dlogp_pflow[-1].squeeze()\n",
    "    xf_pflow = grab(xfs_pflow[-1].squeeze())\n",
    "    \n",
    "    return xf_sde, xf_pflow, logpx_pflow\n",
    "\n",
    "def make_plots(\n",
    "    b: torch.nn.Module,\n",
    "    s: torch.nn.Module,\n",
    "    interpolant: stochastic_interpolant.Interpolant,\n",
    "    n_save: int,\n",
    "    n_step: int,\n",
    "    likelihood_bs: int,\n",
    "    counter: int,\n",
    "    eps: float,\n",
    "    data_dict: dict,\n",
    "    base,\n",
    "    device\n",
    "):\n",
    "    \"\"\"Make plots to visualize samples and evolution of the likelihood.\"\"\"\n",
    "    try:\n",
    "        xf_sde, xf_pflow, logpx_pflow = compute_likelihoods(\n",
    "            b, s, interpolant, n_save, n_step, eps, likelihood_bs, base, device\n",
    "        )\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "        print(f\"EPOCH: {counter}\")\n",
    "        \n",
    "        # Plot losses\n",
    "        epochs = np.arange(len(data_dict['losses']))\n",
    "        axes[0].plot(epochs, data_dict['losses'], label=\"Total\")\n",
    "        axes[0].plot(epochs, data_dict['v_losses'], label=\"Velocity\")\n",
    "        axes[0].plot(epochs, data_dict['s_losses'], label=\"Score\")\n",
    "        if len(data_dict['i_losses']) > 0:\n",
    "            axes[0].plot(epochs, data_dict['i_losses'], label=\"Interpolant\", linestyle='--')\n",
    "        axes[0].set_title(\"Losses\")\n",
    "        axes[0].set_xlabel(\"Epoch\")\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot SDE samples\n",
    "        axes[1].scatter(xf_sde[:, 0], xf_sde[:, 1], alpha=0.3, s=5)\n",
    "        axes[1].set_xlim(-5, 5)\n",
    "        axes[1].set_ylim(-6.5, 6.5)\n",
    "        axes[1].set_title(\"Samples from SDE\")\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot PFlow samples\n",
    "        logpx_np = grab(torch.exp(logpx_pflow))\n",
    "        axes[2].scatter(xf_pflow[:, 0], xf_pflow[:, 1], alpha=0.3, s=5,\n",
    "                       c=logpx_np, cmap='viridis')\n",
    "        axes[2].set_xlim(-5, 5)\n",
    "        axes[2].set_ylim(-6.5, 6.5)\n",
    "        axes[2].set_title(\"Samples from PFlow\")\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot log-likelihood\n",
    "        if len(data_dict['logps_pflow']) > 0:\n",
    "            axes[3].plot(epochs, data_dict['logps_pflow'], label='PFlow', color='purple')\n",
    "        axes[3].set_title(r\"$\\log p$ from PFlow\")\n",
    "        axes[3].set_xlabel(\"Epoch\")\n",
    "        axes[3].legend()\n",
    "        axes[3].grid(True, alpha=0.3)\n",
    "        \n",
    "        fig.suptitle(f\"Nonlinear SI with Neural Flows (Îµ = {eps:.2f})\", fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in plotting: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Target Distribution (Checker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 2\n",
    "\n",
    "def target(bs, device=None):\n",
    "    \"\"\"Generate samples from checker distribution\"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    x1 = torch.rand(bs, device=device) * 4 - 2\n",
    "    x2_ = torch.rand(bs, device=device) - torch.randint(2, (bs,), device=device).float() * 2\n",
    "    x2 = x2_ + (torch.floor(x1) % 2)\n",
    "    return (torch.cat([x1[:, None], x2[:, None]], 1) * 2)\n",
    "\n",
    "# Generate and visualize target samples\n",
    "target_samples = grab(target(10000, device))\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plt.hist2d(target_samples[:, 0], target_samples[:, 1], bins=100, range=[[-4, 4], [-4, 4]], cmap='viridis')\n",
    "plt.colorbar(label='Density')\n",
    "plt.title(\"Checker Target Distribution\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Target sample shape: {target_samples.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Base Distribution (Gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_loc = torch.zeros(ndim)\n",
    "base_var = torch.ones(ndim)\n",
    "base = prior.SimpleNormal(base_loc, 1.0 * base_var)\n",
    "\n",
    "# Move base to device\n",
    "if device.type == 'cuda':\n",
    "    base.loc = base.loc.to(device)\n",
    "    base.scale = base.scale.to(device)\n",
    "\n",
    "base_samples = grab(base(10000).to(device))\n",
    "\n",
    "# Visualize base vs target\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(base_samples[:, 0], base_samples[:, 1], alpha=0.2, s=1, label='Base (Gaussian)')\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.title(\"Base Distribution\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(target_samples[:, 0], target_samples[:, 1], alpha=0.2, s=1, label='Target (Checker)', color='orange')\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.title(\"Target Distribution\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Base vs Target Distributions\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Linear Interpolant First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First test with standard linear interpolant to ensure everything works\n",
    "print(\"Testing with linear interpolant first...\")\n",
    "\n",
    "path = 'one-sided-linear'\n",
    "linear_interpolant = stochastic_interpolant.Interpolant(path=path, gamma_type=None)\n",
    "\n",
    "# Test interpolation\n",
    "test_x0 = base(10).to(device)\n",
    "test_x1 = target(10, device)\n",
    "test_t = torch.tensor(0.5, device=device)\n",
    "\n",
    "try:\n",
    "    xt = linear_interpolant.calc_xt(test_t, test_x0, test_x1)\n",
    "    print(f\"Linear interpolation successful. Output shape: {xt.shape if isinstance(xt, torch.Tensor) else xt[0].shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in linear interpolation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Nonlinear Stochastic Interpolant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create nonlinear interpolant with neural flows\n",
    "print(\"Initializing nonlinear interpolant...\")\n",
    "\n",
    "path = 'one-sided-linear'  # One-sided interpolant with Gaussian base\n",
    "interpolant = NonlinearInterpolantV2(\n",
    "    dim=ndim,\n",
    "    path=path,\n",
    "    gamma_type=None,  # No stochastic component for one-sided\n",
    "    n_layers=2,       # Start with fewer layers for stability\n",
    "    hidden_dims=64,   # Smaller network for debugging\n",
    "    n_bins=4,         # Fewer bins initially\n",
    "    tail_bound=3.0,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Initialized Nonlinear Interpolant with {sum(p.numel() for p in interpolant.parameters())} parameters\")\n",
    "\n",
    "# Test the nonlinear interpolant\n",
    "try:\n",
    "    test_x0 = base(10).to(device)\n",
    "    test_x1 = target(10, device)\n",
    "    test_t = torch.tensor(0.5, device=device)\n",
    "    \n",
    "    xt = interpolant.calc_xt(test_t, test_x0, test_x1)\n",
    "    print(f\"Nonlinear interpolation successful. Output shape: {xt.shape if isinstance(xt, torch.Tensor) else xt[0].shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in nonlinear interpolation: {e}\")\n",
    "    print(\"Falling back to linear interpolant\")\n",
    "    interpolant = linear_interpolant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions for velocity and score\n",
    "loss_fn_v = stochastic_interpolant.make_loss(\n",
    "    method='shared', \n",
    "    interpolant=interpolant, \n",
    "    loss_type='one-sided-v'\n",
    ")\n",
    "\n",
    "loss_fn_s = stochastic_interpolant.make_loss(\n",
    "    method='shared', \n",
    "    interpolant=interpolant, \n",
    "    loss_type='one-sided-s'\n",
    ")\n",
    "\n",
    "print(\"Loss functions created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Neural Networks for Velocity and Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network architecture\n",
    "hidden_sizes = [128, 128, 128]  # Smaller network for debugging\n",
    "in_size = ndim + 1  # dimensions + time\n",
    "out_size = ndim\n",
    "inner_act = 'relu'\n",
    "final_act = 'none'\n",
    "\n",
    "# Create velocity and score networks\n",
    "velocity_net = itf.fabrics.make_fc_net(\n",
    "    hidden_sizes=hidden_sizes,\n",
    "    in_size=in_size,\n",
    "    out_size=out_size,\n",
    "    inner_act=inner_act,\n",
    "    final_act=final_act\n",
    ").to(device)\n",
    "\n",
    "score_net = itf.fabrics.make_fc_net(\n",
    "    hidden_sizes=hidden_sizes,\n",
    "    in_size=in_size,\n",
    "    out_size=out_size,\n",
    "    inner_act=inner_act,\n",
    "    final_act=final_act\n",
    ").to(device)\n",
    "\n",
    "print(f\"Velocity network parameters: {sum(p.numel() for p in velocity_net.parameters())}\")\n",
    "print(f\"Score network parameters: {sum(p.numel() for p in score_net.parameters())}\")\n",
    "\n",
    "# Test the networks\n",
    "try:\n",
    "    test_x = torch.randn(5, ndim, device=device)\n",
    "    test_t = torch.tensor(0.5, device=device)\n",
    "    v_out = velocity_net(test_x, test_t)\n",
    "    s_out = score_net(test_x, test_t)\n",
    "    print(f\"Network test successful. Velocity output: {v_out.shape}, Score output: {s_out.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error testing networks: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer for nonlinear SI\n",
    "# Check if we're using nonlinear interpolant\n",
    "use_nonlinear = isinstance(interpolant, NonlinearInterpolantV2)\n",
    "\n",
    "if use_nonlinear:\n",
    "    print(\"Using NonlinearSITrainerV2 with max-min optimization\")\n",
    "    trainer = NonlinearSITrainerV2(\n",
    "        interpolant=interpolant,\n",
    "        velocity_net=velocity_net,\n",
    "        score_net=score_net,\n",
    "        lr_interpolant=5e-5,  # Very small learning rate for interpolant\n",
    "        lr_velocity=1e-3,     # Learning rate for velocity network\n",
    "        lr_score=1e-3,        # Learning rate for score network\n",
    "        device=device\n",
    "    )\n",
    "else:\n",
    "    print(\"Using standard training (linear interpolant)\")\n",
    "    # Standard optimizers for linear case\n",
    "    opt_velocity = torch.optim.Adam(velocity_net.parameters(), lr=1e-3)\n",
    "    opt_score = torch.optim.Adam(score_net.parameters(), lr=1e-3)\n",
    "    sched_velocity = torch.optim.lr_scheduler.StepLR(opt_velocity, step_size=1500, gamma=0.4)\n",
    "    sched_score = torch.optim.lr_scheduler.StepLR(opt_score, step_size=1500, gamma=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "eps = torch.tensor(0.5, device=device)  # Noise level for SDE\n",
    "n_epochs = 2000              # Total training epochs\n",
    "batch_size = 500             # Smaller batch size for debugging\n",
    "plot_bs = 1000               # Batch size for plotting\n",
    "metrics_freq = 50            # How often to compute metrics\n",
    "plot_freq = 200              # How often to plot\n",
    "n_save = 10                  # Number of checkpoints for SDE\n",
    "n_step = 50                  # Fewer SDE steps for speed\n",
    "n_inner_steps = 1            # Inner minimization steps\n",
    "train_interpolant_after = 500  # Start training interpolant after this many epochs\n",
    "\n",
    "# Data storage\n",
    "data_dict = {\n",
    "    'losses': [],\n",
    "    'v_losses': [],\n",
    "    's_losses': [],\n",
    "    'i_losses': [],\n",
    "    'logps_pflow': [],\n",
    "}\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  - Epochs: {n_epochs}\")\n",
    "print(f\"  - Batch size: {batch_size}\")\n",
    "print(f\"  - Device: {device}\")\n",
    "print(f\"  - Using nonlinear interpolant: {use_nonlinear}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # Generate batch\n",
    "    x0 = base(batch_size).to(device)\n",
    "    x1 = target(batch_size, device)\n",
    "    \n",
    "    if use_nonlinear:\n",
    "        # Training with nonlinear interpolant\n",
    "        # Only train interpolant after initial training\n",
    "        train_interpolant = (epoch > train_interpolant_after)\n",
    "        \n",
    "        losses = trainer.train_step(\n",
    "            x0=x0,\n",
    "            x1=x1,\n",
    "            loss_fn_v=loss_fn_v,\n",
    "            loss_fn_s=loss_fn_s,\n",
    "            n_inner_steps=n_inner_steps,\n",
    "            train_interpolant=train_interpolant\n",
    "        )\n",
    "    else:\n",
    "        # Standard training for linear interpolant\n",
    "        opt_velocity.zero_grad()\n",
    "        opt_score.zero_grad()\n",
    "        \n",
    "        ts = torch.rand(batch_size, device=device)\n",
    "        \n",
    "        loss_v = loss_fn_v(velocity_net, x0, x1, ts, interpolant)\n",
    "        loss_s = loss_fn_s(score_net, x0, x1, ts, interpolant)\n",
    "        \n",
    "        loss_v.backward()\n",
    "        loss_s.backward()\n",
    "        \n",
    "        opt_velocity.step()\n",
    "        opt_score.step()\n",
    "        sched_velocity.step()\n",
    "        sched_score.step()\n",
    "        \n",
    "        losses = {\n",
    "            'velocity': loss_v.item(),\n",
    "            'score': loss_s.item(),\n",
    "            'interpolant': 0.0,\n",
    "            'total': loss_v.item() + loss_s.item()\n",
    "        }\n",
    "    \n",
    "    # Log metrics\n",
    "    if (epoch - 1) % metrics_freq == 0:\n",
    "        data_dict['losses'].append(losses['total'])\n",
    "        data_dict['v_losses'].append(losses['velocity'])\n",
    "        data_dict['s_losses'].append(losses['score'])\n",
    "        data_dict['i_losses'].append(losses['interpolant'])\n",
    "        \n",
    "        # Compute likelihood (optional, can be slow)\n",
    "        if epoch > 100:  # Skip early epochs for speed\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    _, _, logpx_pflow = compute_likelihoods(\n",
    "                        velocity_net, score_net, interpolant, \n",
    "                        n_save, n_step, eps, min(batch_size, 100), base, device\n",
    "                    )\n",
    "                    data_dict['logps_pflow'].append(grab(logpx_pflow).mean())\n",
    "            except Exception as e:\n",
    "                print(f\"Error computing likelihood: {e}\")\n",
    "                data_dict['logps_pflow'].append(0.0)\n",
    "    \n",
    "    # Progress printing\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch:4d} | \"\n",
    "              f\"Loss: {losses['total']:.4f} | \"\n",
    "              f\"V: {losses['velocity']:.4f} | \"\n",
    "              f\"S: {losses['score']:.4f} | \"\n",
    "              f\"I: {losses['interpolant']:.4f}\")\n",
    "    \n",
    "    # Plotting\n",
    "    if (epoch - 1) % plot_freq == 0 and epoch > 1:\n",
    "        make_plots(\n",
    "            velocity_net, score_net, interpolant,\n",
    "            n_save, n_step, plot_bs, epoch, eps, data_dict, base, device\n",
    "        )\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final samples\n",
    "print(\"Generating final samples...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Generate samples using trained model\n",
    "    n_final_samples = 5000\n",
    "    x0_final = base(n_final_samples).to(device)\n",
    "    \n",
    "    try:\n",
    "        # PFlow sampling\n",
    "        pflow = stochastic_interpolant.PFlowIntegrator(\n",
    "            b=velocity_net,\n",
    "            method='dopri5',\n",
    "            interpolant=interpolant,\n",
    "            n_step=5\n",
    "        )\n",
    "        xf_pflow, logp_pflow = pflow.rollout(x0_final)\n",
    "        xf_pflow_np = grab(xf_pflow[-1].squeeze())\n",
    "        \n",
    "        # SDE sampling (smaller batch)\n",
    "        sde_flow = stochastic_interpolant.SDEIntegrator(\n",
    "            b=velocity_net,\n",
    "            s=score_net,\n",
    "            eps=eps,\n",
    "            interpolant=interpolant,\n",
    "            n_save=1,\n",
    "            n_step=50\n",
    "        )\n",
    "        xf_sde = sde_flow.rollout_forward(x0_final[:500])\n",
    "        xf_sde_np = grab(xf_sde[-1].squeeze())\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating samples: {e}\")\n",
    "        xf_pflow_np = grab(x0_final)\n",
    "        xf_sde_np = grab(x0_final[:500])\n",
    "\n",
    "# Create final visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Target distribution\n",
    "axes[0, 0].hist2d(target_samples[:, 0], target_samples[:, 1], \n",
    "                  bins=50, range=[[-4, 4], [-4, 4]], cmap='viridis')\n",
    "axes[0, 0].set_title(\"Target Distribution\", fontsize=12)\n",
    "axes[0, 0].set_xlabel(\"x\")\n",
    "axes[0, 0].set_ylabel(\"y\")\n",
    "\n",
    "# PFlow samples\n",
    "axes[0, 1].hist2d(xf_pflow_np[:, 0], xf_pflow_np[:, 1], \n",
    "                  bins=50, range=[[-4, 4], [-4, 4]], cmap='viridis')\n",
    "axes[0, 1].set_title(\"PFlow Samples\", fontsize=12)\n",
    "axes[0, 1].set_xlabel(\"x\")\n",
    "axes[0, 1].set_ylabel(\"y\")\n",
    "\n",
    "# SDE samples\n",
    "axes[0, 2].hist2d(xf_sde_np[:, 0], xf_sde_np[:, 1], \n",
    "                  bins=50, range=[[-4, 4], [-4, 4]], cmap='viridis')\n",
    "axes[0, 2].set_title(\"SDE Samples\", fontsize=12)\n",
    "axes[0, 2].set_xlabel(\"x\")\n",
    "axes[0, 2].set_ylabel(\"y\")\n",
    "\n",
    "# Training curves\n",
    "epochs = np.arange(len(data_dict['losses'])) * metrics_freq\n",
    "\n",
    "axes[1, 0].plot(epochs, data_dict['losses'], label='Total', linewidth=2)\n",
    "axes[1, 0].plot(epochs, data_dict['v_losses'], label='Velocity', alpha=0.7)\n",
    "axes[1, 0].plot(epochs, data_dict['s_losses'], label='Score', alpha=0.7)\n",
    "axes[1, 0].set_title(\"Training Losses\", fontsize=12)\n",
    "axes[1, 0].set_xlabel(\"Epoch\")\n",
    "axes[1, 0].set_ylabel(\"Loss\")\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Interpolant loss\n",
    "if use_nonlinear and len(data_dict['i_losses']) > 0:\n",
    "    axes[1, 1].plot(epochs, data_dict['i_losses'], color='green', linewidth=2)\n",
    "    axes[1, 1].set_title(\"Interpolant Loss\", fontsize=12)\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, \"Linear Interpolant\\n(No adaptation)\", \n",
    "                   ha='center', va='center', fontsize=12)\n",
    "    axes[1, 1].set_title(\"Interpolant Loss\", fontsize=12)\n",
    "axes[1, 1].set_xlabel(\"Epoch\")\n",
    "axes[1, 1].set_ylabel(\"Loss\")\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Log-likelihood\n",
    "if len(data_dict['logps_pflow']) > 0:\n",
    "    axes[1, 2].plot(epochs[:len(data_dict['logps_pflow'])], \n",
    "                   data_dict['logps_pflow'], color='purple', linewidth=2)\n",
    "axes[1, 2].set_title(\"Log-Likelihood Evolution\", fontsize=12)\n",
    "axes[1, 2].set_xlabel(\"Epoch\")\n",
    "axes[1, 2].set_ylabel(r\"$\\log p$\")\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Stochastic Interpolant Results\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Statistics:\")\n",
    "if len(data_dict['losses']) > 0:\n",
    "    print(f\"  - Final total loss: {data_dict['losses'][-1]:.4f}\")\n",
    "if len(data_dict['logps_pflow']) > 0:\n",
    "    print(f\"  - Final log-likelihood: {data_dict['logps_pflow'][-1]:.4f}\")\n",
    "if use_nonlinear:\n",
    "    print(f\"  - Number of NSF parameters: {sum(p.numel() for p in interpolant.parameters())}\")\n",
    "print(f\"  - Total trainable parameters: {sum(p.numel() for p in velocity_net.parameters()) + sum(p.numel() for p in score_net.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained models\n",
    "import os\n",
    "\n",
    "save_models = False  # Set to True to save\n",
    "\n",
    "if save_models:\n",
    "    save_dir = \"../saved_models/nonlinear_si/\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    checkpoint = {\n",
    "        'velocity_state': velocity_net.state_dict(),\n",
    "        'score_state': score_net.state_dict(),\n",
    "        'training_history': data_dict,\n",
    "        'config': {\n",
    "            'dim': ndim,\n",
    "            'path': path,\n",
    "            'eps': eps.item(),\n",
    "            'n_epochs': n_epochs,\n",
    "            'use_nonlinear': use_nonlinear\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if use_nonlinear:\n",
    "        checkpoint['interpolant_state'] = interpolant.T_t.state_dict()\n",
    "    \n",
    "    torch.save(checkpoint, os.path.join(save_dir, 'si_checker.pt'))\n",
    "    print(f\"Models saved to {save_dir}\")\n",
    "else:\n",
    "    print(\"Skipping model saving\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}