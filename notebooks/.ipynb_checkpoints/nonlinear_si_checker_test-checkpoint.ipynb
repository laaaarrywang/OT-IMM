{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear Stochastic Interpolant with Neural Spline Flows\n",
    "## Testing on Checker Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from typing import Tuple, Any\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import interflow as itf\n",
    "import interflow.prior as prior\n",
    "import interflow.fabrics\n",
    "import interflow.stochastic_interpolant as stochastic_interpolant\n",
    "from interflow.nonlinear_stochastic_interpolant import (\n",
    "    NonlinearInterpolant, NonlinearSITrainer\n",
    ")\n",
    "from torch import autograd\n",
    "\n",
    "# Check CUDA availability\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA available, setting default tensor residence to GPU.')\n",
    "    device = torch.device('cuda')\n",
    "    itf.util.set_torch_device('cuda')\n",
    "else:\n",
    "    print('No CUDA device found, using CPU.')\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab(var):\n",
    "    \"\"\"Take a tensor off the gpu and convert it to a numpy array on the CPU.\"\"\"\n",
    "    return var.detach().cpu().numpy()\n",
    "\n",
    "def compute_likelihoods(\n",
    "    b: torch.nn.Module,\n",
    "    s: torch.nn.Module,\n",
    "    interpolant: stochastic_interpolant.Interpolant,\n",
    "    n_save: int,\n",
    "    n_step: int,\n",
    "    eps: float,\n",
    "    bs: int\n",
    ") -> Tuple[torch.tensor, torch.tensor, torch.tensor]:\n",
    "    \"\"\"Draw samples from the probability flow and SDE models, and compute likelihoods.\"\"\"\n",
    "    \n",
    "    sde_flow = stochastic_interpolant.SDEIntegrator(\n",
    "        b=b, s=s, eps=eps, interpolant=interpolant, n_save=n_save, n_likelihood=1, n_step=n_step\n",
    "    )\n",
    "    pflow = stochastic_interpolant.PFlowIntegrator(\n",
    "        b=b,  \n",
    "        method='dopri5', \n",
    "        interpolant=interpolant,\n",
    "        n_step=3\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x0_tests = base(bs)\n",
    "        xfs_sde = sde_flow.rollout_forward(x0_tests)\n",
    "        xf_sde = grab(xfs_sde[-1].squeeze())\n",
    "        x0s_sdeflow, _ = sde_flow.rollout_likelihood(xfs_sde[-1])\n",
    "    \n",
    "    logp0 = base.log_prob(x0_tests)\n",
    "    xfs_pflow, dlogp_pflow = pflow.rollout(x0_tests)\n",
    "    logpx_pflow = logp0 + dlogp_pflow[-1].squeeze()\n",
    "    xf_pflow = grab(xfs_pflow[-1].squeeze())\n",
    "    \n",
    "    return xf_sde, xf_pflow, logpx_pflow\n",
    "\n",
    "def make_plots(\n",
    "    b: torch.nn.Module,\n",
    "    s: torch.nn.Module,\n",
    "    interpolant: stochastic_interpolant.Interpolant,\n",
    "    n_save: int,\n",
    "    n_step: int,\n",
    "    likelihood_bs: int,\n",
    "    counter: int,\n",
    "    eps: float,\n",
    "    data_dict: dict\n",
    "):\n",
    "    \"\"\"Make plots to visualize samples and evolution of the likelihood.\"\"\"\n",
    "    xf_sde, xf_pflow, logpx_pflow = compute_likelihoods(\n",
    "        b, s, interpolant, n_save, n_step, eps, likelihood_bs\n",
    "    )\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    print(f\"EPOCH: {counter}\")\n",
    "    \n",
    "    # Plot losses\n",
    "    epochs = np.arange(len(data_dict['losses']))\n",
    "    axes[0].plot(epochs, data_dict['losses'], label=\"Total\")\n",
    "    axes[0].plot(epochs, data_dict['v_losses'], label=\"Velocity\")\n",
    "    axes[0].plot(epochs, data_dict['s_losses'], label=\"Score\")\n",
    "    axes[0].plot(epochs, data_dict['i_losses'], label=\"Interpolant\", linestyle='--')\n",
    "    axes[0].set_title(\"Losses\")\n",
    "    axes[0].set_xlabel(\"Epoch\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot SDE samples\n",
    "    axes[1].scatter(xf_sde[:, 0], xf_sde[:, 1], alpha=0.3, s=5)\n",
    "    axes[1].set_xlim(-5, 5)\n",
    "    axes[1].set_ylim(-6.5, 6.5)\n",
    "    axes[1].set_title(\"Samples from SDE\")\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot PFlow samples\n",
    "    axes[2].scatter(xf_pflow[:, 0], xf_pflow[:, 1], alpha=0.3, s=5,\n",
    "                   c=grab(torch.exp(logpx_pflow).detach()), cmap='viridis')\n",
    "    axes[2].set_xlim(-5, 5)\n",
    "    axes[2].set_ylim(-6.5, 6.5)\n",
    "    axes[2].set_title(\"Samples from PFlow\")\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot log-likelihood\n",
    "    axes[3].plot(epochs, data_dict['logps_pflow'], label='PFlow', color='purple')\n",
    "    axes[3].set_title(r\"$\\log p$ from PFlow\")\n",
    "    axes[3].set_xlabel(\"Epoch\")\n",
    "    axes[3].legend()\n",
    "    axes[3].grid(True, alpha=0.3)\n",
    "    \n",
    "    fig.suptitle(f\"Nonlinear SI with Neural Spline Flows (Îµ = {eps:.2f})\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Target Distribution (Checker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 2\n",
    "\n",
    "def target(bs):\n",
    "    \"\"\"Generate samples from checker distribution\"\"\"\n",
    "    x1 = torch.rand(bs, device=device) * 4 - 2\n",
    "    x2_ = torch.rand(bs, device=device) - torch.randint(2, (bs,), device=device) * 2\n",
    "    x2 = x2_ + (torch.floor(x1) % 2)\n",
    "    return (torch.cat([x1[:, None], x2[:, None]], 1) * 2)\n",
    "\n",
    "# Generate and visualize target samples\n",
    "target_samples = grab(target(10000))\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plt.hist2d(target_samples[:, 0], target_samples[:, 1], bins=100, range=[[-4, 4], [-4, 4]], cmap='viridis')\n",
    "plt.colorbar(label='Density')\n",
    "plt.title(\"Checker Target Distribution\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Target sample shape: {target_samples.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Base Distribution (Gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_loc = torch.zeros(ndim, device=device)\n",
    "base_var = torch.ones(ndim, device=device)\n",
    "base = prior.SimpleNormal(base_loc, 1.0 * base_var)\n",
    "base_samples = grab(base(10000))\n",
    "\n",
    "# Visualize base vs target\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(base_samples[:, 0], base_samples[:, 1], alpha=0.2, s=1, label='Base (Gaussian)')\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.title(\"Base Distribution\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(target_samples[:, 0], target_samples[:, 1], alpha=0.2, s=1, label='Target (Checker)', color='orange')\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.title(\"Target Distribution\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Base vs Target Distributions\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Nonlinear Stochastic Interpolant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create nonlinear interpolant with neural spline flows\n",
    "path = 'one-sided-linear'  # One-sided interpolant with Gaussian base\n",
    "interpolant = NonlinearInterpolant(\n",
    "    dim=ndim,\n",
    "    path=path,\n",
    "    gamma_type=None,  # No stochastic component for one-sided\n",
    "    n_layers=4,       # Number of coupling layers in NSF\n",
    "    hidden_dims=128,  # Hidden dimensions in conditioner networks\n",
    "    n_bins=8,         # Number of bins for rational quadratic splines\n",
    "    tail_bound=3.0,   # Tail bound for splines\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Initialized Nonlinear Interpolant with {sum(p.numel() for p in interpolant.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions for velocity and score\n",
    "loss_fn_v = stochastic_interpolant.make_loss(\n",
    "    method='shared', \n",
    "    interpolant=interpolant, \n",
    "    loss_type='one-sided-v'\n",
    ")\n",
    "\n",
    "loss_fn_s = stochastic_interpolant.make_loss(\n",
    "    method='shared', \n",
    "    interpolant=interpolant, \n",
    "    loss_type='one-sided-s'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Neural Networks for Velocity and Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network architecture\n",
    "hidden_sizes = [256, 256, 256, 256]\n",
    "in_size = ndim + 1  # dimensions + time\n",
    "out_size = ndim\n",
    "inner_act = 'relu'\n",
    "final_act = 'none'\n",
    "\n",
    "# Create velocity and score networks\n",
    "velocity_net = itf.fabrics.make_fc_net(\n",
    "    hidden_sizes=hidden_sizes,\n",
    "    in_size=in_size,\n",
    "    out_size=out_size,\n",
    "    inner_act=inner_act,\n",
    "    final_act=final_act\n",
    ").to(device)\n",
    "\n",
    "score_net = itf.fabrics.make_fc_net(\n",
    "    hidden_sizes=hidden_sizes,\n",
    "    in_size=in_size,\n",
    "    out_size=out_size,\n",
    "    inner_act=inner_act,\n",
    "    final_act=final_act\n",
    ").to(device)\n",
    "\n",
    "print(f\"Velocity network parameters: {sum(p.numel() for p in velocity_net.parameters())}\")\n",
    "print(f\"Score network parameters: {sum(p.numel() for p in score_net.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Trainer with Max-Min Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer for nonlinear SI with max-min optimization\n",
    "trainer = NonlinearSITrainer(\n",
    "    interpolant=interpolant,\n",
    "    velocity_net=velocity_net,\n",
    "    score_net=score_net,\n",
    "    lr_interpolant=1e-4,  # Learning rate for interpolant (NSF)\n",
    "    lr_velocity=2e-3,     # Learning rate for velocity network\n",
    "    lr_score=2e-3,        # Learning rate for score network\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized with max-min optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "eps = 0.5           # Noise level for SDE\n",
    "n_epochs = 5000     # Total training epochs\n",
    "batch_size = 2000   # Batch size\n",
    "plot_bs = 5000      # Batch size for plotting\n",
    "metrics_freq = 50   # How often to compute metrics\n",
    "plot_freq = 500     # How often to plot\n",
    "n_save = 10         # Number of checkpoints for SDE\n",
    "n_step = 100        # Number of SDE steps\n",
    "n_inner_steps = 2   # Inner minimization steps in max-min\n",
    "\n",
    "# Data storage\n",
    "data_dict = {\n",
    "    'losses': [],\n",
    "    'v_losses': [],\n",
    "    's_losses': [],\n",
    "    'i_losses': [],\n",
    "    'logps_pflow': [],\n",
    "}\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  - Epochs: {n_epochs}\")\n",
    "print(f\"  - Batch size: {batch_size}\")\n",
    "print(f\"  - Inner steps: {n_inner_steps}\")\n",
    "print(f\"  - Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # Generate batch\n",
    "    x0 = base(batch_size)\n",
    "    x1 = target(batch_size)\n",
    "    \n",
    "    # Training step with max-min optimization\n",
    "    losses = trainer.train_step(\n",
    "        x0=x0,\n",
    "        x1=x1,\n",
    "        loss_fn_v=loss_fn_v,\n",
    "        loss_fn_s=loss_fn_s,\n",
    "        n_inner_steps=n_inner_steps\n",
    "    )\n",
    "    \n",
    "    # Log metrics\n",
    "    if (epoch - 1) % metrics_freq == 0:\n",
    "        data_dict['losses'].append(losses['total'])\n",
    "        data_dict['v_losses'].append(losses['velocity'])\n",
    "        data_dict['s_losses'].append(losses['score'])\n",
    "        data_dict['i_losses'].append(losses['interpolant'])\n",
    "        \n",
    "        # Compute likelihood\n",
    "        with torch.no_grad():\n",
    "            _, _, logpx_pflow = compute_likelihoods(\n",
    "                velocity_net, score_net, interpolant, \n",
    "                n_save, n_step, eps, batch_size\n",
    "            )\n",
    "            data_dict['logps_pflow'].append(grab(logpx_pflow).mean())\n",
    "    \n",
    "    # Progress printing\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch:4d} | \"\n",
    "              f\"Loss: {losses['total']:.4f} | \"\n",
    "              f\"V: {losses['velocity']:.4f} | \"\n",
    "              f\"S: {losses['score']:.4f} | \"\n",
    "              f\"I: {losses['interpolant']:.4f}\")\n",
    "    \n",
    "    # Plotting\n",
    "    if (epoch - 1) % plot_freq == 0 and epoch > 1:\n",
    "        make_plots(\n",
    "            velocity_net, score_net, interpolant,\n",
    "            n_save, n_step, plot_bs, epoch, eps, data_dict\n",
    "        )\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final samples\n",
    "print(\"Generating final samples...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Generate samples using trained model\n",
    "    n_final_samples = 10000\n",
    "    x0_final = base(n_final_samples)\n",
    "    \n",
    "    # PFlow sampling\n",
    "    pflow = stochastic_interpolant.PFlowIntegrator(\n",
    "        b=velocity_net,\n",
    "        method='dopri5',\n",
    "        interpolant=interpolant,\n",
    "        n_step=5\n",
    "    )\n",
    "    xf_pflow, logp_pflow = pflow.rollout(x0_final)\n",
    "    xf_pflow_np = grab(xf_pflow[-1].squeeze())\n",
    "    \n",
    "    # SDE sampling\n",
    "    sde_flow = stochastic_interpolant.SDEIntegrator(\n",
    "        b=velocity_net,\n",
    "        s=score_net,\n",
    "        eps=eps,\n",
    "        interpolant=interpolant,\n",
    "        n_save=1,\n",
    "        n_step=100\n",
    "    )\n",
    "    xf_sde = sde_flow.rollout_forward(x0_final[:1000])  # Smaller batch for SDE\n",
    "    xf_sde_np = grab(xf_sde[-1].squeeze())\n",
    "\n",
    "# Create final visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Target distribution\n",
    "axes[0, 0].hist2d(target_samples[:, 0], target_samples[:, 1], \n",
    "                  bins=50, range=[[-4, 4], [-4, 4]], cmap='viridis')\n",
    "axes[0, 0].set_title(\"Target Distribution\", fontsize=12)\n",
    "axes[0, 0].set_xlabel(\"x\")\n",
    "axes[0, 0].set_ylabel(\"y\")\n",
    "\n",
    "# PFlow samples\n",
    "axes[0, 1].hist2d(xf_pflow_np[:, 0], xf_pflow_np[:, 1], \n",
    "                  bins=50, range=[[-4, 4], [-4, 4]], cmap='viridis')\n",
    "axes[0, 1].set_title(\"PFlow Samples\", fontsize=12)\n",
    "axes[0, 1].set_xlabel(\"x\")\n",
    "axes[0, 1].set_ylabel(\"y\")\n",
    "\n",
    "# SDE samples\n",
    "axes[0, 2].hist2d(xf_sde_np[:, 0], xf_sde_np[:, 1], \n",
    "                  bins=50, range=[[-4, 4], [-4, 4]], cmap='viridis')\n",
    "axes[0, 2].set_title(\"SDE Samples\", fontsize=12)\n",
    "axes[0, 2].set_xlabel(\"x\")\n",
    "axes[0, 2].set_ylabel(\"y\")\n",
    "\n",
    "# Training curves\n",
    "epochs = np.arange(len(data_dict['losses'])) * metrics_freq\n",
    "\n",
    "axes[1, 0].plot(epochs, data_dict['losses'], label='Total', linewidth=2)\n",
    "axes[1, 0].plot(epochs, data_dict['v_losses'], label='Velocity', alpha=0.7)\n",
    "axes[1, 0].plot(epochs, data_dict['s_losses'], label='Score', alpha=0.7)\n",
    "axes[1, 0].set_title(\"Training Losses\", fontsize=12)\n",
    "axes[1, 0].set_xlabel(\"Epoch\")\n",
    "axes[1, 0].set_ylabel(\"Loss\")\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Interpolant loss\n",
    "axes[1, 1].plot(epochs, data_dict['i_losses'], color='green', linewidth=2)\n",
    "axes[1, 1].set_title(\"Interpolant Loss (Adversarial)\", fontsize=12)\n",
    "axes[1, 1].set_xlabel(\"Epoch\")\n",
    "axes[1, 1].set_ylabel(\"Loss\")\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Log-likelihood\n",
    "axes[1, 2].plot(epochs, data_dict['logps_pflow'], color='purple', linewidth=2)\n",
    "axes[1, 2].set_title(\"Log-Likelihood Evolution\", fontsize=12)\n",
    "axes[1, 2].set_xlabel(\"Epoch\")\n",
    "axes[1, 2].set_ylabel(r\"$\\log p$\")\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Nonlinear Stochastic Interpolant Results\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Statistics:\")\n",
    "print(f\"  - Final total loss: {data_dict['losses'][-1]:.4f}\")\n",
    "print(f\"  - Final log-likelihood: {data_dict['logps_pflow'][-1]:.4f}\")\n",
    "print(f\"  - Number of NSF parameters: {sum(p.numel() for p in interpolant.parameters())}\")\n",
    "print(f\"  - Total trainable parameters: {sum(p.numel() for p in interpolant.parameters()) + sum(p.numel() for p in velocity_net.parameters()) + sum(p.numel() for p in score_net.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Interpolation Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned nonlinear interpolation path\n",
    "print(\"Visualizing interpolation path...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Sample a few points\n",
    "    n_viz = 100\n",
    "    x0_viz = base(n_viz)\n",
    "    x1_viz = target(n_viz)\n",
    "    \n",
    "    # Time points along the path\n",
    "    t_points = torch.linspace(0, 1, 11, device=device)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 6, figsize=(18, 6))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, t in enumerate(t_points):\n",
    "        if i >= 11:\n",
    "            axes[i].axis('off')\n",
    "            continue\n",
    "            \n",
    "        # Compute interpolated points\n",
    "        t_batch = t.expand(n_viz)\n",
    "        \n",
    "        # For one-sided interpolant\n",
    "        if interpolant.path in ['one-sided-linear', 'one-sided-trig']:\n",
    "            xt = interpolant.It(t_batch, x0_viz, x1_viz)\n",
    "        else:\n",
    "            xt, _ = interpolant.calc_xt(t_batch, x0_viz, x1_viz)\n",
    "        \n",
    "        xt_np = grab(xt)\n",
    "        \n",
    "        # Plot\n",
    "        axes[i].scatter(xt_np[:, 0], xt_np[:, 1], alpha=0.5, s=10)\n",
    "        axes[i].set_xlim(-5, 5)\n",
    "        axes[i].set_ylim(-5, 5)\n",
    "        axes[i].set_title(f\"t = {t.item():.1f}\", fontsize=10)\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        if i == 0:\n",
    "            axes[i].set_ylabel(\"y\")\n",
    "        if i >= 6:\n",
    "            axes[i].set_xlabel(\"x\")\n",
    "    \n",
    "    # Turn off the last axis\n",
    "    axes[-1].axis('off')\n",
    "    \n",
    "    plt.suptitle(\"Nonlinear Interpolation Path from Base to Target\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained models\n",
    "import os\n",
    "\n",
    "save_dir = \"../saved_models/nonlinear_si/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save model states\n",
    "torch.save({\n",
    "    'interpolant_state': interpolant.T_t.state_dict(),\n",
    "    'velocity_state': velocity_net.state_dict(),\n",
    "    'score_state': score_net.state_dict(),\n",
    "    'training_history': data_dict,\n",
    "    'config': {\n",
    "        'dim': ndim,\n",
    "        'path': path,\n",
    "        'n_layers': 4,\n",
    "        'hidden_dims': 128,\n",
    "        'n_bins': 8,\n",
    "        'eps': eps,\n",
    "        'n_epochs': n_epochs\n",
    "    }\n",
    "}, os.path.join(save_dir, 'nonlinear_si_checker.pt'))\n",
    "\n",
    "print(f\"Models saved to {save_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}